{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '/home/jimmyyao/repos/rev-stable-diffusion-2/notebooks',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python39.zip',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/jimmyyao/.local/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/repos/segment-anything',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/certifi-2022.12.7-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/wheel-0.40.0-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/setuptools-67.6.1-py3.9.egg',\n",
       " '/home/jimmyyao/repos/GenerativeImage2Text']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add to path\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.insert(0, '../')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from samgit.inference import prepare_model, forward\n",
    "\n",
    "PRETRAINED_MODEL = \"../notebooks/output/SAMGIT/epoch1/model.pt\"\n",
    "SAM_MODEL_TYPE=\"vit_h\"\n",
    "SAM_CHECKPOINT=\"../models/sam_vit_h_4b8939.pth\"\n",
    "# TOKENIZER_PRETRAINED=\"/kaggle/input/bert-base-uncased\"\n",
    "model, mask_generator, tokenizer, param = prepare_model(PRETRAINED_MODEL,\n",
    "                                                        sam_model_type=SAM_MODEL_TYPE,\n",
    "                                                        # tokenizer_pretrained=TOKENIZER_PRETRAINED,\n",
    "                                                        sam_checkpoint=SAM_CHECKPOINT\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_img(img_path: str) -> str:\n",
    "    result = forward(model, mask_generator, tokenizer, param, img_path)\n",
    "    return tokenizer.decode(result['predictions'][0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "def caption_img_batch(img_paths: list) -> list:\n",
    "    result = forward(model, mask_generator, tokenizer, param, img_paths)\n",
    "    return [tokenizer.decode(i.tolist(), skip_special_tokens=True) for i in result['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>hyper realistic photo of very friendly and dys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>ramen carved out of fractal rose ebony, in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>ultrasaurus holding a black bean taco in the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a thundering retro robot crane inks on parchme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait painting of a shimmering greek hero, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt\n",
       "0  20057f34d  hyper realistic photo of very friendly and dys...\n",
       "1  227ef0887  ramen carved out of fractal rose ebony, in the...\n",
       "2  92e911621  ultrasaurus holding a black bean taco in the w...\n",
       "3  a4e1c55a9  a thundering retro robot crane inks on parchme...\n",
       "4  c98f79f71  portrait painting of a shimmering greek hero, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "import os\n",
    "PART1_IMGS_PATH = \"../datasets/sampleeval\"\n",
    "\n",
    "#csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../datasets/sampleeval/prompts.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/sampleeval/20057f34d.png',\n",
       " '../datasets/sampleeval/227ef0887.png',\n",
       " '../datasets/sampleeval/92e911621.png',\n",
       " '../datasets/sampleeval/a4e1c55a9.png',\n",
       " '../datasets/sampleeval/c98f79f71.png',\n",
       " '../datasets/sampleeval/d8edf2e40.png',\n",
       " '../datasets/sampleeval/f27825b2c.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths = [os.path.join(PART1_IMGS_PATH, df.iloc[idx]['imgId']+\".png\") for idx in range(len(df))]\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from samgit.train import forward as trian_forward\n",
    "# from generativeimage2text.common import Config\n",
    "\n",
    "# cfg = {\n",
    "#         'crop_region_extend_in_datatransform': 4,\n",
    "#         'data_normalize': 'clip',\n",
    "#         'train_crop_size': 224,\n",
    "#         'input_small_scale': 0.8,\n",
    "#         'no_color_jitter': True,\n",
    "#         'no_flip': True,\n",
    "#         'no_aspect_dist': True,\n",
    "#         'interpolation': 'bicubic',\n",
    "#         'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "#         # 'min_size_range32': [224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "#         'patch_size': 16,\n",
    "#         'train_transform': 'vitp',\n",
    "#     }\n",
    "# cfg = Config(cfg, {})\n",
    "\n",
    "# trian_forward(model, mask_generator, tokenizer, cfg, param, img_paths, df['prompt'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(result, tokenizer):\n",
    "    captions = tokenizer.batch_decode(result['predictions'], skip_special_tokens=True)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = forward(model, mask_generator, tokenizer, param, img_paths, main_crop_size=160)\n",
    "# generate(result, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_predictions = []\n",
    "# for img_path in img_paths:\n",
    "#     r = forward(model, mask_generator, tokenizer, param, img_path, main_crop_size=160)\n",
    "#     new_predictions.append(r['predictions'][0])\n",
    "# generate({'predictions':new_predictions}, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = forward(model, mask_generator, tokenizer, param, img_paths, main_crop_size=224)\n",
    "# generate(result, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat playing with a ball']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "r = forward(model, mask_generator, tokenizer, param, os.path.join(PART1_IMGS_PATH, f\"2.png\"), main_crop_size=224)\n",
    "generate(r, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a black hole in the middle of the world, digital art',\n",
       " 'a wood carving of a man',\n",
       " 'a dinosaur in the middle of a forest',\n",
       " 'a drawing of a robot',\n",
       " 'painting of a man with a dragonfly head',\n",
       " 'an astronaut walking through a cherry blossom garden',\n",
       " 'a man eating pizza at a mcdonalds']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions = []\n",
    "for img_path in img_paths:\n",
    "    r = forward(model, mask_generator, tokenizer, param, img_path, main_crop_size=224)\n",
    "    new_predictions.append(r['predictions'][0])\n",
    "\n",
    "gen_captions = generate({'predictions':new_predictions}, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gen_caption'] = gen_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gen_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>hyper realistic photo of very friendly and dys...</td>\n",
       "      <td>a black hole in the middle of the world, digit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>ramen carved out of fractal rose ebony, in the...</td>\n",
       "      <td>a wood carving of a man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>ultrasaurus holding a black bean taco in the w...</td>\n",
       "      <td>a dinosaur in the middle of a forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a thundering retro robot crane inks on parchme...</td>\n",
       "      <td>a drawing of a robot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait painting of a shimmering greek hero, ...</td>\n",
       "      <td>painting of a man with a dragonfly head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d8edf2e40</td>\n",
       "      <td>an astronaut standing on a engaging white rose...</td>\n",
       "      <td>an astronaut walking through a cherry blossom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f27825b2c</td>\n",
       "      <td>Kaggle employee Phil at a donut shop ordering ...</td>\n",
       "      <td>a man eating pizza at a mcdonalds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt   \n",
       "0  20057f34d  hyper realistic photo of very friendly and dys...  \\\n",
       "1  227ef0887  ramen carved out of fractal rose ebony, in the...   \n",
       "2  92e911621  ultrasaurus holding a black bean taco in the w...   \n",
       "3  a4e1c55a9  a thundering retro robot crane inks on parchme...   \n",
       "4  c98f79f71  portrait painting of a shimmering greek hero, ...   \n",
       "5  d8edf2e40  an astronaut standing on a engaging white rose...   \n",
       "6  f27825b2c  Kaggle employee Phil at a donut shop ordering ...   \n",
       "\n",
       "                                         gen_caption  \n",
       "0  a black hole in the middle of the world, digit...  \n",
       "1                            a wood carving of a man  \n",
       "2               a dinosaur in the middle of a forest  \n",
       "3                               a drawing of a robot  \n",
       "4            painting of a man with a dragonfly head  \n",
       "5  an astronaut walking through a cherry blossom ...  \n",
       "6                  a man eating pizza at a mcdonalds  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hyper realistic photo of very friendly and dystopian crater',\n",
       " 'ramen carved out of fractal rose ebony, in the style of hudson river school',\n",
       " 'ultrasaurus holding a black bean taco in the woods, near an identical cheneosaurus',\n",
       " 'a thundering retro robot crane inks on parchment with a droopy french bulldog',\n",
       " 'portrait painting of a shimmering greek hero, next to a loud frill-necked lizard',\n",
       " 'an astronaut standing on a engaging white rose, in the midst of by ivory cherry blossoms',\n",
       " 'Kaggle employee Phil at a donut shop ordering all the best donuts, with a speech bubble that proclaims \"Donuts. It\\'s what\\'s for dinner!\"']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.DataFrame({'imgId': df['imgId'].to_list(), 'gen_caption': df['gen_caption'].to_list()})\n",
    "# new_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a black hole in the middle of the world, digital art',\n",
       " 'a wood carving of a man',\n",
       " 'a dinosaur in the middle of a forest',\n",
       " 'a drawing of a robot',\n",
       " 'painting of a man with a dragonfly head',\n",
       " 'an astronaut walking through a cherry blossom garden',\n",
       " 'a man eating pizza at a mcdonalds']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gen_caption'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4559473395347595\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "prompt_embeddings = st_model.encode(df['prompt'].to_list())\n",
    "gen_caption_embeddings = st_model.encode(df['gen_caption'].to_list())\n",
    "\n",
    "print(cos(torch.tensor(prompt_embeddings), torch.tensor(gen_caption_embeddings)).mean().item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM Crops Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a circle in the middle of the earth'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def generate_caption(image: Image):\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True, max_length=40)\n",
    "\n",
    "image = Image.open(img_paths[0]).convert(\"RGB\")\n",
    "generate_caption(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arafed view of a circular hole in the middle of a desert'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
    "\n",
    "# unconditional image captioning\n",
    "from PIL import Image\n",
    "\n",
    "def generate_caption_blip(image: Image):\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(**inputs)\n",
    "    return processor.decode(out[0], skip_special_tokens=True, max_length=40)\n",
    "\n",
    "image = Image.open(img_paths[0]).convert(\"RGB\")\n",
    "generate_caption_blip(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_image_crops(image, mask_generator, top_n_bbox, h_w_ratio=2.5):\n",
    "    masks = mask_generator.generate(image)\n",
    "    sorted_anns = sorted(masks, key=(lambda x: x['predicted_iou']), reverse=True)\n",
    "    \n",
    "    crops = []\n",
    "    for ann in sorted_anns:\n",
    "        x0, y0, x1, y1 = ann['bbox']\n",
    "        if x0 > x1: \n",
    "            x0, x1 = x1, x0\n",
    "        if y0 > y1: \n",
    "            y0, y1 = y1, y0\n",
    "        \n",
    "        height, width = y1-y0, x1-x0\n",
    "        # print(crop.shape)\n",
    "        ratio = height/(width+1e-6)\n",
    "        if width > height:\n",
    "            ratio = width/(height+1e-6)\n",
    "        \n",
    "        if height < 10 or width < 10:\n",
    "            continue\n",
    "        \n",
    "        if ratio > h_w_ratio: # throw out images that are too thin, they don't provide much info\n",
    "            continue\n",
    "\n",
    "        crops.append(image[y0:y1, x0:x1, :])\n",
    "        if len(crops) >= top_n_bbox:\n",
    "            break\n",
    "        \n",
    "    return crops\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "import cv2\n",
    "def caption_img_sam(image: str, caption_main=True) -> str:\n",
    "    with torch.no_grad():\n",
    "        with autocast(dtype=torch.float16):\n",
    "            image = cv2.imread(image)\n",
    "            crops = sam_image_crops(image, mask_generator, 2, h_w_ratio=2.5) # top 2 iou with h_w_ratio\n",
    "            \n",
    "            crop_captions = []\n",
    "            for crop in crops:\n",
    "                if 0 in crop.shape:\n",
    "                    continue\n",
    "                crop = Image.fromarray(crop[:,:,::-1])\n",
    "                crop_captions.append(generate_caption_blip(crop))\n",
    "    # return \"\"\n",
    "    # print(len(crop_captions))\n",
    "    \n",
    "        image_PIL = Image.fromarray(image[:,:,::-1])\n",
    "        caption = generate_caption_blip(image_PIL)\n",
    "    \n",
    "    \n",
    "    crop_captions = \", \".join(crop_captions)\n",
    "    if crop_captions == \"\":\n",
    "        return caption\n",
    "    else:\n",
    "        return f\"{caption}, {crop_captions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[step 1|391895] A man in a red shirt and a red hat is on a motorcycle on a hill side.\t:   0%|          | 0/40504 [00:00<?, ?it/s]/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "[step 76|192440] A shelf with hygiene products in a bathroom.a pedestal sink.\t:   0%|          | 75/40504 [01:07<10:31:48,  1.07it/s]                                        \n",
      "[step 76|192440] A shelf with hygiene products in a bathroom.          \n",
      "[step 185|521634] There is an image of an outdoor area. treet signs on a pole\t:   0%|          | 184/40504 [02:45<10:06:45,  1.11it/s]                                      \n",
      "\n",
      "[step 185|521634] There is an image of an outdoor area. ]                   \n",
      "\n",
      "[step 239|495612] a street sign attached to a post with apartment buildings in the background:31,  1.06it/s]                                                                    \n",
      "[step 239|495612] a street sign attached to a post with apartment buildings in the background\n",
      "[step 783|301102] One tennis racket is place on top of the other one.2%|▏         | 782/40504 [11:27<8:45:39,  1.26it/s]                                                                                \n",
      "[step 783|301102] One tennis racket is place on top of the other one.\n",
      "[step 1139|123511] the orange is in a bowl on the tablenic devices on a desk.\t:   3%|▎         | 1138/40504 [16:42<9:04:08,  1.21it/s]                                          \n",
      "[step 1139|123511] the orange is in a bowl on the tables]                     \n",
      "[step 1585|574796] An orange, chocolate, cracker, and piece of lettuce on a plate:   4%|▍         | 1584/40504 [22:59<9:21:30,  1.16it/s]                                                                                                           \n",
      "[step 1585|574796] An orange, chocolate, cracker, and piece of lettuce on a plate\n",
      "[step 1694|292789] A skateboard is sitting in a tree outside.  4%|▍         | 1693/40504 [24:31<7:36:30,  1.42it/s]                                                                        \n",
      "[step 1694|292789] A skateboard is sitting in a tree outside.\n",
      "[step 1727|2867] four skiers ready to ski down a snowy mountain|▍         | 1726/40504 [24:59<9:40:07,  1.11it/s]                                      \n",
      "[step 1727|2867] four skiers ready to ski down a snowy mountain\n",
      "[step 2030|99828] There is something captured here in this photo.   5%|▌         | 2029/40504 [29:24<9:08:58,  1.17it/s]                                                          \n",
      "[step 2030|99828] There is something captured here in this photo.\n",
      "[step 2130|555050] a shop a car traffic lights and buildings get out\t:   5%|▌         | 2129/40504 [30:50<9:11:52,  1.16it/s]                                           \n",
      "[step 2130|555050] a shop a car traffic lights and buildings    \n",
      "[step 2402|110587] Human being having fun and enjoying some life. ock.\t:   6%|▌         | 2401/40504 [34:41<9:22:34,  1.13it/s]                                                                                                                                                                         \n",
      "[step 2402|110587] Human being having fun and enjoying some life. \n",
      "[step 2664|385196] Boys and girls ring on skateboards at a park.e skateboarding. \t:   7%|▋         | 2663/40504 [38:26<9:18:59,  1.13it/s]                                                    \n",
      "[step 2664|385196] Boys and girls ring on skateboards at a park.         \n",
      "[step 2724|466567] a donut being used as an ornament for a chistmas tree         | 2723/40504 [39:18<9:16:07,  1.13it/s]                                                 \n",
      "[step 2724|466567] a donut being used as an ornament for a chistmas tree\n",
      "[step 3526|49777]  A storm trooper is in the sand under an umbrella.\t:   9%|▊         | 3525/40504 [50:56<8:27:36,  1.21it/s]                                                          \n",
      "[step 3526|49777] s traveling down a road with lots of traffic.\t:   9%|▊         | 3525/40504 [50:56<8:27:36,  1.21it/s]\n",
      "[step 3954|352552] A person flying a kite near a basketball hoop. ▉         | 3953/40504 [57:12<9:20:56,  1.09it/s]                                                                                                                                         \n",
      "[step 3954|352552] A person flying a kite near a basketball hoop. \n",
      "[step 4472|472067] A big full view of several people gathering.led trunck\t:  11%|█         | 4471/40504 [1:04:40<8:26:38,  1.19it/s]                                                                       \n",
      "\n",
      "[step 4472|472067] A big full view of several people gathering.    \n",
      "\n",
      "[step 4803|566529] A man is lying down and holding a stuffed blue teddy bear.2/40504 [1:09:18<8:25:48,  1.18it/s]                                                                                                  \n",
      "[step 4803|566529] A man is lying down and holding a stuffed blue teddy bear.\n",
      "[step 4834|468171] A stack of pillows on a hotel bedo each other.\t:  12%|█▏        | 4833/40504 [1:09:44<8:45:25,  1.13it/s]                                            \n",
      "[step 4834|468171] A stack of pillows on a hotel bed13it/s]            \n",
      "[step 5133|5247] Two people on skis going through the woods. resort.\t:  13%|█▎        | 5132/40504 [1:14:04<8:41:03,  1.13it/s]                                                      \n",
      "[step 5133|5247] Two people on skis going through the woods.      \n",
      "[step 5586|517388] man looking bewildered looking over all the suitcasesres.\t:  14%|█▍        | 5585/40504 [1:20:32<8:50:33,  1.10it/s]                                                             \n",
      "[step 5586|517388] man looking bewildered looking over all the suitcases\n",
      "[step 6117|471691] The eye piece of the eyeglasses is inside the glass vase.building \t:  15%|█▌        | 6116/40504 [1:28:13<8:41:56,  1.10it/s]                                                                    \n",
      "[step 6117|471691] The eye piece of the eyeglasses is inside the glass vase.\n",
      "[step 6270|508302] An individual is doing something that is uncommonly entrancing.    | 6269/40504 [1:30:27<8:12:18,  1.16it/s]                                      \n",
      "[step 6270|508302] An individual is doing something that is uncommonly entrancing. \n",
      "[step 6329|259452] A picture of an open air zone that looks incredible.:  16%|█▌        | 6328/40504 [1:31:18<8:49:51,  1.08it/s]                                                       \n",
      "[step 6329|259452] A picture of an open air zone that looks incredible.\n",
      "[step 6578|489899] This is a picture of a zoo outdoor. ear ends. \t:  16%|█▌        | 6577/40504 [1:34:54<7:40:12,  1.23it/s]                                                                   \n",
      "[step 6578|489899] This is a picture of a zoo outdoor. t/s]         \n",
      "[step 6693|463722] Photograph of an outdoor arena that looks neat.ed against a light sky.\t:  17%|█▋        | 6692/40504 [1:36:32<8:16:41,  1.13it/s]                                     \n",
      "[step 6693|463722] Photograph of an outdoor arena that looks neat.               \n",
      "[step 6836|484485] An individual is doing something that is uncommonly entrancing.   | 6835/40504 [1:38:32<6:53:09,  1.36it/s]                                                    \n",
      "[step 6836|484485] An individual is doing something that is uncommonly entrancing. \n",
      "[step 7286|120021] People looking at a motorcycle that is on display. ▊        | 7285/40504 [1:45:04<8:05:05,  1.14it/s]                                                                                     \n",
      "[step 7286|120021] People looking at a motorcycle that is on display. \n",
      "[step 7774|181677] An individual is in the open view in the image. stairs while a group of people watch.\t:  19%|█▉        | 7773/40504 [1:51:54<8:13:36,  1.11it/s]                                                         \n",
      "[step 7774|181677] An individual is in the open view in the image.                              \n",
      "[step 8026|298978] Here is a soul in the image alone.  a smart phone sitting next to her.\t:  20%|█▉        | 8025/40504 [1:55:35<8:02:03,  1.12it/s]                                                                                                                                                                      \n",
      "[step 8026|298978] Here is a soul in the image alone. it/s]                                  \n",
      "[step 8117|527887] A lady talking on a cellphone walking down a street holding a   20%|██        | 8116/40504 [1:56:54<8:08:05,  1.11it/s]                                                                                                                                                                        \n",
      "[step 8117|527887] A lady talking on a cellphone walking down a street holding a \n",
      "[step 8315|532463] A picture of an outside region that appears incredible. ething\t:  21%|██        | 8314/40504 [1:59:47<6:27:30,  1.38it/s]                                                                                                    \n",
      "[step 8315|532463] A picture of an outside region that appears incredible. \n",
      "[step 8457|232550] There is a picture of an outside area.y intersection.\t:  21%|██        | 8456/40504 [2:01:48<7:20:30,  1.21it/s]                                                                       \n",
      "[step 8457|232550] There is a picture of an outside area.s]              \n",
      "[step 8742|149252] A photo of an outdoor with many things in the scene.d.\t:  22%|██▏       | 8741/40504 [2:05:50<7:39:20,  1.15it/s]                                                                            \n",
      "[step 8742|149252] A photo of an outdoor with many things in the scene.\n",
      "[step 9028|140661] There is a huge room with furniture and things inside. | 9027/40504 [2:09:55<7:51:51,  1.11it/s]                                                                       \n",
      "[step 9028|140661] There is a huge room with furniture and things inside.\n",
      "[step 9115|443723] Miicrosoft  manufactures the latest electronic devices in the market▎       | 9114/40504 [2:11:11<7:44:56,  1.13it/s]                                                       \n",
      "[step 9115|443723] Miicrosoft  manufactures the latest electronic devices in the market\n",
      "[step 9165|113867] A picture of an open air zone that looks incredible.|██▎       | 9164/40504 [2:11:55<7:38:01,  1.14it/s]                                      \n",
      "[step 9165|113867] A picture of an open air zone that looks incredible.\n",
      "[step 10058|428231] The living room has three chairs and three tables.█▍       | 10057/40504 [2:24:43<7:27:09,  1.13it/s]                                                                                                                                    \n",
      "[step 10058|428231] The living room has three chairs and three tables.\n",
      "[step 10343|390348] A bike parked in the middle of a street covered up with a cloth . 10342/40504 [2:28:53<7:28:04,  1.12it/s]                                                                             \n",
      "[step 10343|390348] A bike parked in the middle of a street covered up with a cloth .\n",
      "[step 10457|361948] two people sitting on a purple bench and a little bird standing on the grass [2:30:27<6:20:19,  1.32it/s]                                                                     \n",
      "[step 10457|361948] two people sitting on a purple bench and a little bird standing on the grass\n",
      "[step 10718|565341] Two small dogs on the grass next to a drivewaymputer.\t:  26%|██▋       | 10717/40504 [2:34:12<7:13:33,  1.15it/s]                                                           \n",
      "[step 10718|565341] Two small dogs on the grass next to a driveway\n",
      "[step 10744|566656] A big full view of several people gathering.ky area.\t:  27%|██▋       | 10743/40504 [2:34:32<6:20:51,  1.30it/s]                      \n",
      "[step 10744|566656] A big full view of several people gathering.   \n",
      "[step 10868|184425] A table that is covered with wine glasses.\t:  27%|██▋       | 10867/40504 [2:36:16<6:55:43,  1.19it/s]                                       \n",
      "[step 10868|184425] A table that is covered with wine glasses.\n",
      "[step 11132|290618] A Woman on a tennis court prepares to return a ball.   27%|██▋       | 11131/40504 [2:40:07<5:58:22,  1.37it/s]                                                                \n",
      "[step 11132|290618] A Woman on a tennis court prepares to return a ball. \n",
      "[step 11143|107389] a female reaching in the air with a tennis racketnning\t:  28%|██▊       | 11142/40504 [2:40:16<6:17:11,  1.30it/s]                     \n",
      "[step 11143|107389] a female reaching in the air with a tennis racket\n",
      "[step 12182|293837] There is a room with various items in the picture.█       | 12181/40504 [2:55:07<7:02:13,  1.12it/s]                                                                                                        \n",
      "[step 12182|293837] There is a room with various items in the picture.\n",
      "[step 12326|72810] an image of a bike parked next to a benchnk.\t:  30%|███       | 12325/40504 [2:57:16<7:12:23,  1.09it/s]                                                   \n",
      "[step 12326|72810] an image of a bike parked next to a bench  \n",
      "[step 12641|348829] A stop sign at the intersection of two snowy streets with mountains in the distance.\t:  31%|███       | 12640/40504 [3:01:47<6:40:43,  1.16it/s]                                         \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mean must have 1 elements if it is an iterable, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m image_id, image_file, cap \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     42\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[step \u001b[39m\u001b[39m{\u001b[39;00mpbar\u001b[39m.\u001b[39mn\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mimage_id[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mcap[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     gen_caption \u001b[39m=\u001b[39m caption_img_sam(image_file[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     45\u001b[0m     captions_val2014_fakecap_results\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m: image_id[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem(), \u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m: gen_caption})\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m (pbar\u001b[39m.\u001b[39mn \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[149], line 41\u001b[0m, in \u001b[0;36mcaption_img_sam\u001b[0;34m(image, caption_main)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m             crop \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(crop[:,:,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 41\u001b[0m             crop_captions\u001b[39m.\u001b[39mappend(generate_caption_blip(crop))\n\u001b[1;32m     42\u001b[0m \u001b[39m# return \"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# print(len(crop_captions))\u001b[39;00m\n\u001b[1;32m     45\u001b[0m image_PIL \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image[:,:,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[134], line 12\u001b[0m, in \u001b[0;36mgenerate_caption_blip\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_caption_blip\u001b[39m(image: Image):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# image = Image.open(image_path).convert(\"RGB\")\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39;49mimage, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m processor\u001b[39m.\u001b[39mdecode(out[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/processing_blip.py:101\u001b[0m, in \u001b[0;36mBlipProcessor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m text_encoding\n\u001b[1;32m    100\u001b[0m \u001b[39m# add pixel_values\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m encoding_image_processor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     text_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    105\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m    106\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    121\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/image_processing_blip.py:281\u001b[0m, in \u001b[0;36mBlipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 281\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(image\u001b[39m=\u001b[39mimage, mean\u001b[39m=\u001b[39mimage_mean, std\u001b[39m=\u001b[39mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    283\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    285\u001b[0m encoded_outputs \u001b[39m=\u001b[39m BatchFeature(data\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/image_processing_blip.py:281\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    278\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 281\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    283\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    285\u001b[0m encoded_outputs \u001b[39m=\u001b[39m BatchFeature(data\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/image_processing_blip.py:181\u001b[0m, in \u001b[0;36mBlipImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m            The channel dimension format of the image. If not provided, it will be the same as the input image.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(image, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/image_transforms.py:369\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mean, Iterable):\n\u001b[1;32m    368\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mean) \u001b[39m!=\u001b[39m num_channels:\n\u001b[0;32m--> 369\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean must have \u001b[39m\u001b[39m{\u001b[39;00mnum_channels\u001b[39m}\u001b[39;00m\u001b[39m elements if it is an iterable, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(mean)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     mean \u001b[39m=\u001b[39m [mean] \u001b[39m*\u001b[39m num_channels\n",
      "\u001b[0;31mValueError\u001b[0m: mean must have 1 elements if it is an iterable, got 3"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# get data\n",
    "import os\n",
    "IMGS_PATH = \"../datasets/cococaps/val2014\"\n",
    "\n",
    "#json\n",
    "import json\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../datasets/cococaps/captions_val2014.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class COCOEvalDataset(Dataset):\n",
    "    def __init__(self, data, db_base_path):\n",
    "        self.data = data\n",
    "        self.captions = {a['image_id']:a for a in data['annotations']}\n",
    "        \n",
    "        self.db_base_path = db_base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['images'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_file = os.path.join(self.db_base_path, self.data['images'][idx]['file_name'])\n",
    "        image_id = self.data['images'][idx]['id']\n",
    "        cap = self.captions[data['images'][idx]['id']]['caption']\n",
    "        return image_id, image_file, cap\n",
    "    \n",
    "\n",
    "cocoeval_dataset = COCOEvalDataset(data, IMGS_PATH)\n",
    "cocoeval_dataloader = DataLoader(cocoeval_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    \n",
    "pbar = tqdm(cocoeval_dataloader)\n",
    "\n",
    "# fake_val2014 = {'image_id':-1, \"caption\":\"fake caption\"}\n",
    "\n",
    "captions_val2014_fakecap_results = []\n",
    "for image_id, image_file, cap in pbar:\n",
    "    pbar.set_description(f\"[step {pbar.n + 1}|{image_id[0].item()}] {cap[0]}\\t\")\n",
    "    \n",
    "    gen_caption = caption_img_sam(image_file[0])\n",
    "    captions_val2014_fakecap_results.append({'image_id': image_id[0].item(), 'caption': gen_caption})\n",
    "    if (pbar.n + 1) % 10000 == 0:\n",
    "        with open(f'../datasets/cococaps/captions_sam+blip{pbar.n + 1}.json', 'w') as f:\n",
    "            json.dump(captions_val2014_fakecap_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29520it [4:01:23,  1.16it/s]                             \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m image_id, image_file, cap \u001b[39m=\u001b[39m cocoeval_dataset[i]\n\u001b[1;32m      5\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[step \u001b[39m\u001b[39m{\u001b[39;00mpbar\u001b[39m.\u001b[39mn\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mimage_id\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mcap\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m gen_caption \u001b[39m=\u001b[39m caption_img_sam(image_file)\n\u001b[1;32m      8\u001b[0m captions_val2014_fakecap_results\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m: image_id, \u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m: gen_caption})\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m (pbar\u001b[39m.\u001b[39mn \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[214], line 44\u001b[0m, in \u001b[0;36mcaption_img_sam\u001b[0;34m(image, caption_main)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m             crop \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(crop[:,:,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 44\u001b[0m             crop_captions\u001b[39m.\u001b[39mappend(generate_caption_blip(crop))\n\u001b[1;32m     45\u001b[0m \u001b[39m# return \"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# print(len(crop_captions))\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     image_PIL \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image[:,:,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[134], line 13\u001b[0m, in \u001b[0;36mgenerate_caption_blip\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_caption_blip\u001b[39m(image: Image):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# image = Image.open(image_path).convert(\"RGB\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimage, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m processor\u001b[39m.\u001b[39mdecode(out[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip.py:1081\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m input_ids[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtext_config\u001b[39m.\u001b[39mbos_token_id\n\u001b[1;32m   1079\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1082\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m   1083\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtext_config\u001b[39m.\u001b[39;49msep_token_id,\n\u001b[1;32m   1084\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtext_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1085\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1086\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mimage_embeds,\n\u001b[1;32m   1087\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1088\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs,\n\u001b[1;32m   1089\u001b[0m )\n\u001b[1;32m   1091\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/generation/utils.py:1437\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1432\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1433\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1434\u001b[0m         )\n\u001b[1;32m   1436\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1438\u001b[0m         input_ids,\n\u001b[1;32m   1439\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1440\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1441\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1442\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1443\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1444\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1445\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1446\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1447\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1448\u001b[0m     )\n\u001b[1;32m   1450\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1451\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/generation/utils.py:2248\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2245\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2247\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2248\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2249\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2250\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2251\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2252\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2253\u001b[0m )\n\u001b[1;32m   2255\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2256\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:865\u001b[0m, in \u001b[0;36mBlipTextLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m    866\u001b[0m     input_ids,\n\u001b[1;32m    867\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    868\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    869\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    870\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    871\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    872\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    873\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    874\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    875\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    876\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    877\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    878\u001b[0m     is_decoder\u001b[39m=\u001b[39;49mis_decoder,\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    881\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    882\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:775\u001b[0m, in \u001b[0;36mBlipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     embedding_output \u001b[39m=\u001b[39m encoder_embeds\n\u001b[0;32m--> 775\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    776\u001b[0m     embedding_output,\n\u001b[1;32m    777\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    778\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    779\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    780\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    781\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    782\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    783\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    784\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    785\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    787\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    788\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:433\u001b[0m, in \u001b[0;36mBlipTextEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    424\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    425\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    434\u001b[0m         hidden_states,\n\u001b[1;32m    435\u001b[0m         attention_mask,\n\u001b[1;32m    436\u001b[0m         layer_head_mask,\n\u001b[1;32m    437\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    438\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    439\u001b[0m         past_key_value,\n\u001b[1;32m    440\u001b[0m         output_attentions,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:350\u001b[0m, in \u001b[0;36mBlipTextLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m present_key_value \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossattention(\n\u001b[1;32m    351\u001b[0m         attention_output,\n\u001b[1;32m    352\u001b[0m         attention_mask,\n\u001b[1;32m    353\u001b[0m         head_mask,\n\u001b[1;32m    354\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    355\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    356\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m     attention_output \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    359\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# add cross attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:267\u001b[0m, in \u001b[0;36mBlipTextAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ):\n\u001b[0;32m--> 267\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    268\u001b[0m         hidden_states,\n\u001b[1;32m    269\u001b[0m         attention_mask,\n\u001b[1;32m    270\u001b[0m         head_mask,\n\u001b[1;32m    271\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    272\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    273\u001b[0m         past_key_value,\n\u001b[1;32m    274\u001b[0m         output_attentions,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    277\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:204\u001b[0m, in \u001b[0;36mBlipTextSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     attention_probs_dropped \u001b[39m=\u001b[39m attention_probs_dropped \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 204\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs_dropped, value_layer)\n\u001b[1;32m    206\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    207\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pbar2 = tqdm(range(12652, len(cocoeval_dataset)), initial=12652)\n",
    "for i in pbar2:\n",
    "\n",
    "    image_id, image_file, cap = cocoeval_dataset[i]\n",
    "    pbar.set_description(f\"[step {pbar.n + 1}|{image_id}] {cap}\\t\")\n",
    "\n",
    "    gen_caption = caption_img_sam(image_file)\n",
    "    captions_val2014_fakecap_results.append({'image_id': image_id, 'caption': gen_caption})\n",
    "    if (pbar.n + 1) % 10000 == 0:\n",
    "        with open(f'../datasets/cococaps/captions_sam+blip{pbar.n + 1}.json', 'w') as f:\n",
    "            json.dump(captions_val2014_fakecap_results, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/cococaps/captions_sam+blip.json', 'w') as f:\n",
    "    json.dump(captions_val2014_fakecap_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arafed view of a circular hole in the middle of a desert, arafed view of a circular hole with a small amount of dirt, arafed view of a circular hole in the middle of a desert',\n",
       " 'a close up of a wooden plate with a swirl design on it, a close up of a wooden plate with a flower on it',\n",
       " 'cartoon dinosaur with a piece of cheese in its mouth in a forest, cartoon of a blue rat with a piece of cheese in his hand, cartoon dinosaur with a piece of cheese in its mouth',\n",
       " 'there is a drawing of a robot holding a hammer, there is a drawing of a robot with a bow and arrow, there is a drawing of a robot with a clock on it',\n",
       " 'painting of a man with a lizard on his head and a lizard on his shoulder',\n",
       " 'arafed astronaut walking down a path in a park with cherry trees, astronaut in white spacesuit with american flag on the back, astronaut in white spacesuit with american flag and space shuttle in background',\n",
       " \"there is a man standing in front of a counter with a pizza, there is a man sitting at a table with a cell phone, cartoon of a man with a speech bubble saying it's donuts downs ts\"]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_captions = []\n",
    "for img_path in img_paths:\n",
    "    cap = caption_img_sam(img_path)\n",
    "    gen_captions.append(cap)\n",
    "gen_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gen_caption</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>hyper realistic photo of very friendly and dys...</td>\n",
       "      <td>arafed view of a circular hole in the middle o...</td>\n",
       "      <td>0.203885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>ramen carved out of fractal rose ebony, in the...</td>\n",
       "      <td>a close up of a wooden plate with a swirl desi...</td>\n",
       "      <td>0.253239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>ultrasaurus holding a black bean taco in the w...</td>\n",
       "      <td>cartoon dinosaur with a piece of cheese in its...</td>\n",
       "      <td>0.315348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a thundering retro robot crane inks on parchme...</td>\n",
       "      <td>there is a drawing of a robot holding a hammer...</td>\n",
       "      <td>0.402890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait painting of a shimmering greek hero, ...</td>\n",
       "      <td>painting of a man with a lizard on his head an...</td>\n",
       "      <td>0.571044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d8edf2e40</td>\n",
       "      <td>an astronaut standing on a engaging white rose...</td>\n",
       "      <td>arafed astronaut walking down a path in a park...</td>\n",
       "      <td>0.439187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f27825b2c</td>\n",
       "      <td>Kaggle employee Phil at a donut shop ordering ...</td>\n",
       "      <td>there is a man standing in front of a counter ...</td>\n",
       "      <td>0.010855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt   \n",
       "0  20057f34d  hyper realistic photo of very friendly and dys...  \\\n",
       "1  227ef0887  ramen carved out of fractal rose ebony, in the...   \n",
       "2  92e911621  ultrasaurus holding a black bean taco in the w...   \n",
       "3  a4e1c55a9  a thundering retro robot crane inks on parchme...   \n",
       "4  c98f79f71  portrait painting of a shimmering greek hero, ...   \n",
       "5  d8edf2e40  an astronaut standing on a engaging white rose...   \n",
       "6  f27825b2c  Kaggle employee Phil at a donut shop ordering ...   \n",
       "\n",
       "                                         gen_caption  cosine_similarity  \n",
       "0  arafed view of a circular hole in the middle o...           0.203885  \n",
       "1  a close up of a wooden plate with a swirl desi...           0.253239  \n",
       "2  cartoon dinosaur with a piece of cheese in its...           0.315348  \n",
       "3  there is a drawing of a robot holding a hammer...           0.402890  \n",
       "4  painting of a man with a lizard on his head an...           0.571044  \n",
       "5  arafed astronaut walking down a path in a park...           0.439187  \n",
       "6  there is a man standing in front of a counter ...           0.010855  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gen_caption'] = gen_captions\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4599408507347107\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "prompt_embeddings = st_model.encode(df['prompt'].to_list())\n",
    "gen_caption_embeddings = st_model.encode(df['gen_caption'].to_list())\n",
    "\n",
    "print(cos(torch.tensor(prompt_embeddings), torch.tensor(gen_caption_embeddings)).mean().item())\n",
    "df[\"cosine_similarity\"] = cos(torch.tensor(prompt_embeddings), torch.tensor(gen_caption_embeddings)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gen_caption</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>hyper realistic photo of very friendly and dys...</td>\n",
       "      <td>arafed view of a circular hole in the middle o...</td>\n",
       "      <td>0.409247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>ramen carved out of fractal rose ebony, in the...</td>\n",
       "      <td>a close up of a wooden plate with a swirl desi...</td>\n",
       "      <td>0.234214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>ultrasaurus holding a black bean taco in the w...</td>\n",
       "      <td>cartoon dinosaur with a piece of cheese in its...</td>\n",
       "      <td>0.382151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a thundering retro robot crane inks on parchme...</td>\n",
       "      <td>there is a drawing of a robot holding a hammer...</td>\n",
       "      <td>0.452026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait painting of a shimmering greek hero, ...</td>\n",
       "      <td>painting of a man with a lizard on his head an...</td>\n",
       "      <td>0.705105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d8edf2e40</td>\n",
       "      <td>an astronaut standing on a engaging white rose...</td>\n",
       "      <td>arafed astronaut walking down a path in a park...</td>\n",
       "      <td>0.501832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f27825b2c</td>\n",
       "      <td>Kaggle employee Phil at a donut shop ordering ...</td>\n",
       "      <td>there is a man standing in front of a counter ...</td>\n",
       "      <td>0.535012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt   \n",
       "0  20057f34d  hyper realistic photo of very friendly and dys...  \\\n",
       "1  227ef0887  ramen carved out of fractal rose ebony, in the...   \n",
       "2  92e911621  ultrasaurus holding a black bean taco in the w...   \n",
       "3  a4e1c55a9  a thundering retro robot crane inks on parchme...   \n",
       "4  c98f79f71  portrait painting of a shimmering greek hero, ...   \n",
       "5  d8edf2e40  an astronaut standing on a engaging white rose...   \n",
       "6  f27825b2c  Kaggle employee Phil at a donut shop ordering ...   \n",
       "\n",
       "                                         gen_caption  cosine_similarity  \n",
       "0  arafed view of a circular hole in the middle o...           0.409247  \n",
       "1  a close up of a wooden plate with a swirl desi...           0.234214  \n",
       "2  cartoon dinosaur with a piece of cheese in its...           0.382151  \n",
       "3  there is a drawing of a robot holding a hammer...           0.452026  \n",
       "4  painting of a man with a lizard on his head an...           0.705105  \n",
       "5  arafed astronaut walking down a path in a park...           0.501832  \n",
       "6  there is a man standing in front of a counter ...           0.535012  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"SAM+BLIP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev-sd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
