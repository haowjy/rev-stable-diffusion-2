{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '/home/jimmyyao/repos/rev-stable-diffusion-2/notebooks',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python39.zip',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/jimmyyao/.local/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/repos/segment-anything',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/certifi-2022.12.7-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/wheel-0.40.0-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/setuptools-67.6.1-py3.9.egg',\n",
       " '/home/jimmyyao/repos/GenerativeImage2Text']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add to path\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.insert(0, '../')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {}\n",
    "\n",
    "# image_encoder = get_image_encoder(\n",
    "#         param.get('image_encoder_type', 'CLIPViT_B_16'),\n",
    "#         input_resolution=param.get('test_crop_size', 224),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = torch.randn(2, 3, 120, 120)\n",
    "\n",
    "# image_encoder(i1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.rand(768)\n",
    "# t2 = torch.rand(768)\n",
    "# t3 = torch.rand(768)\n",
    "\n",
    "# li = [t1, t2, t3]\n",
    "# # convert li to tensor\n",
    "# t = torch.stack(li)\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    \n",
    "    return info\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    info = get_gpu_utilization()\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 809 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from generativeimage2text.train import (get_image_transform, get_transform_image_norm, \n",
    "                                        get_inception_train_transform, \n",
    "                                        get_data, collate_fn, recursive_to_device)\n",
    "from generativeimage2text.common import Config\n",
    "from generativeimage2text.torch_common import load_state_dict\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from torch.cuda.amp import autocast\n",
    "    \n",
    "from samgit.model import sam_image_crops, get_samgit_model\n",
    "\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_small_transform_vit_default(cfg, crop=24):\n",
    "    default_normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    normalize = get_transform_image_norm(cfg, default_normalize)\n",
    "    transform = get_inception_train_transform(\n",
    "        bgr2rgb=True,\n",
    "        crop_size=crop,\n",
    "        normalize=normalize,\n",
    "        small_scale=cfg.input_small_scale,\n",
    "        no_color_jitter=cfg.no_color_jitter,\n",
    "        no_flip=cfg.no_flip,\n",
    "        no_aspect_dist=cfg.no_aspect_dist,\n",
    "        resize_crop=cfg.resize_crop,\n",
    "        max_size=cfg.train_max_size,\n",
    "        interpolation=cfg.interpolation or Image.BILINEAR,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "def prepare_model(pretrained_path=None, sam_model_type=\"vit_h\", sam_checkpoint = \"../models/sam_vit_h_4b8939.pth\", half=True):\n",
    "    cfg = {\n",
    "        'crop_region_extend_in_datatransform': 4,\n",
    "        'data_normalize': 'clip',\n",
    "        'train_crop_size': 224,\n",
    "        'input_small_scale': 0.8,\n",
    "        'no_color_jitter': True,\n",
    "        'no_flip': True,\n",
    "        'no_aspect_dist': True,\n",
    "        'interpolation': 'bicubic',\n",
    "        'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        # 'min_size_range32': [224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        'patch_size': 16,\n",
    "        'train_transform': 'vitp',\n",
    "    }\n",
    "    cfg = Config(cfg, {})\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    model = get_samgit_model(tokenizer, param)\n",
    "    \n",
    "    # from pre-trained model\n",
    "    if pretrained_path is not None:\n",
    "        checkpoint = torch.load(pretrained_path)['model']\n",
    "        load_state_dict(model, checkpoint)\n",
    "        \n",
    "    # if half:\n",
    "    #     model.half()\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    \n",
    "    # preprocess object crops with sam\n",
    "    sam = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint).half()\n",
    "    # if half:\n",
    "    #     sam.half()\n",
    "    sam.eval()\n",
    "    sam.requires_grad_(False)\n",
    "    sam.to(device='cuda')\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=4,\n",
    "        pred_iou_thresh=0.95,\n",
    "        stability_score_thresh=0.95,\n",
    "    )\n",
    "    \n",
    "    return model, mask_generator, tokenizer, cfg, param\n",
    "\n",
    "def forward(model, mask_generator, tokenizer, cfg, param, image_files, captions, prefixs=None, half=True):\n",
    "    if prefixs is None:\n",
    "        prefixs = [''] * len(captions)\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    image_transform = get_image_transform(cfg)\n",
    "    # resize to 24x24\n",
    "    small_transform = get_small_transform_vit_default(cfg, crop=24)\n",
    "    \n",
    "    for image_file, prefix, target in zip(image_files, prefixs, captions):\n",
    "        data = get_data(image_file, prefix, target,\n",
    "                        tokenizer, image_transform)\n",
    "        all_data.append(data)\n",
    "    data = collate_fn(all_data)\n",
    "    data = recursive_to_device(data, 'cuda')\n",
    "    \n",
    "    logging.info(f\"GPU memory occupied: {get_gpu_utilization().used//1024**2} MB.\")\n",
    "    \n",
    "    batch_img_crops = []\n",
    "    with autocast(dtype=torch.float16 if half else torch.float32):\n",
    "        for img_name in image_files:\n",
    "            image = cv2.imread(img_name)\n",
    "            crops = sam_image_crops(image, mask_generator, param.get('top_n_bbox', 4))\n",
    "            new_crops = []\n",
    "            for crop in crops:\n",
    "                if 0 in crop.shape:\n",
    "                    continue\n",
    "                crop = Image.fromarray(crop)\n",
    "                new_crops.append(small_transform(crop))\n",
    "\n",
    "            if len(new_crops) == 0:\n",
    "                batch_img_crops.append(None)\n",
    "            else:\n",
    "                crops = torch.stack(new_crops)\n",
    "                crops.half()\n",
    "                batch_img_crops.append(crops)\n",
    "            \n",
    "        loss_dict = model(data, batch_img_crops) # hard-coded to cuda\n",
    "    return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>part</th>\n",
       "      <th>new_image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0301-to-0400...</td>\n",
       "      <td>bbc article announcing the beginning of a nucl...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>haunted house ghost bugs dark room grey black</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0601-to-0700...</td>\n",
       "      <td>la llorona serving cunt and being fierce</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0501-to-0600...</td>\n",
       "      <td>Medusa, fantasy horror art, hissing scream, fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/c67d39f8-98f4-439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0101-to-0200...</td>\n",
       "      <td>cleopatra in ancient egypt in front of a pyram...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b5141b36-447c-492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144890</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0701-to-0800...</td>\n",
       "      <td>racionais mc's com traje de astronauta</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/546518b9-74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144891</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0801-to-0900...</td>\n",
       "      <td>dante traveling through the 7 layers of hell</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/02ceb3f5-30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144892</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1801-to-1900...</td>\n",
       "      <td>Spike Spiegel by Kentaro Miura, Charachter Por...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/21f3563b-6d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144893</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>can't sleep. clown will eat me. horror ; night...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/8f13247b-55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144894</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0201-to-0300...</td>\n",
       "      <td>photo of macintosh 1 2 8 k with mouse and keyb...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/3c026626-1a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144895 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name   \n",
       "0       /kaggle/input/diffusiondb-2m-part-0301-to-0400...  \\\n",
       "1       /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "2       /kaggle/input/diffusiondb-2m-part-0601-to-0700...   \n",
       "3       /kaggle/input/diffusiondb-2m-part-0501-to-0600...   \n",
       "4       /kaggle/input/diffusiondb-2m-part-0101-to-0200...   \n",
       "...                                                   ...   \n",
       "144890  /kaggle/input/diffusiondb-2m-part-0701-to-0800...   \n",
       "144891  /kaggle/input/diffusiondb-2m-part-0801-to-0900...   \n",
       "144892  /kaggle/input/diffusiondb-2m-part-1801-to-1900...   \n",
       "144893  /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "144894  /kaggle/input/diffusiondb-2m-part-0201-to-0300...   \n",
       "\n",
       "                                                   prompt  part   \n",
       "0       bbc article announcing the beginning of a nucl...     0  \\\n",
       "1           haunted house ghost bugs dark room grey black     0   \n",
       "2                la llorona serving cunt and being fierce     0   \n",
       "3       Medusa, fantasy horror art, hissing scream, fa...     0   \n",
       "4       cleopatra in ancient egypt in front of a pyram...     0   \n",
       "...                                                   ...   ...   \n",
       "144890             racionais mc's com traje de astronauta     3   \n",
       "144891       dante traveling through the 7 layers of hell     3   \n",
       "144892  Spike Spiegel by Kentaro Miura, Charachter Por...     3   \n",
       "144893  can't sleep. clown will eat me. horror ; night...     3   \n",
       "144894  photo of macintosh 1 2 8 k with mouse and keyb...     3   \n",
       "\n",
       "                                           new_image_name  \n",
       "0       diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...  \n",
       "1       diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...  \n",
       "2       diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...  \n",
       "3       diffusiondb-filtered-0-40000/c67d39f8-98f4-439...  \n",
       "4       diffusiondb-filtered-0-40000/b5141b36-447c-492...  \n",
       "...                                                   ...  \n",
       "144890  diffusiondb-filtered-120001-144894/546518b9-74...  \n",
       "144891  diffusiondb-filtered-120001-144894/02ceb3f5-30...  \n",
       "144892  diffusiondb-filtered-120001-144894/21f3563b-6d...  \n",
       "144893  diffusiondb-filtered-120001-144894/8f13247b-55...  \n",
       "144894  diffusiondb-filtered-120001-144894/3c026626-1a...  \n",
       "\n",
       "[144895 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "import os\n",
    "import pandas as pd\n",
    "DIFFUSIONDB_IMGS_PATH = \"/mnt/d/AiStuff/data/diffusionDB2M-f2/images\"\n",
    "\n",
    "# # csvs\n",
    "# csvs = [\"diffusiondb-filtered-0-40000.csv\", \"diffusiondb-filtered-40001-80000.csv\", \"diffusiondb-filtered-80001-120000.csv\", \"diffusiondb-filtered-120001-144894.csv\"]\n",
    "# dfs = []\n",
    "# for csv in csvs:\n",
    "#     df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, csv))\n",
    "#     dfs.append(df)\n",
    "# df = pd.concat(dfs)\n",
    "\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "class DiffusionDBDataset(Dataset):\n",
    "    def __init__(self, df, db_base_path):\n",
    "        self.df = df\n",
    "        self.db_base_path = db_base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_file = os.path.join(self.db_base_path, row[\"new_image_name\"])\n",
    "        prompt = row[\"prompt\"]\n",
    "        return image_file, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered-0-40000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='train.info.log', encoding='utf-8', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "WORKERS = 2\n",
    "\n",
    "LR = 5e-5 # https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md#learning-rate-notes 1e-5 to 5e-5... although they suggest a schedule\n",
    "\n",
    "SAVE_STEPS = 400\n",
    "\n",
    "SKIP_STEPS = 0\n",
    "PRETRAINED_PATH = f\"output/SAMGIT/epoch1/model.pt\"\n",
    "MODEL_NAME = \"SAMGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1 epoch\n",
    "model, mask_generator, tokenizer, cfg, param = prepare_model(pretrained_path=PRETRAINED_PATH, half=True)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiffusionDBDataset(df, DIFFUSIONDB_IMGS_PATH)\n",
    "train_random_sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_random_sampler, batch_size=BATCH_SIZE, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 4310 MB.\n",
      "memory_allocated: 1812.8544921875 MB, max_memory_allocated: 2402.02197265625 MB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n",
    "print(f\"memory_allocated: {torch.cuda.memory_allocated()/(1024**2)} MB, max_memory_allocated: {torch.cuda.max_memory_allocated()/(1024**2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[step 1132] loss: 4.0250: 100%|██████████| 1132/1132 [3:37:37<00:00, 11.53s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 24.00 GiB total capacity; 20.59 GiB already allocated; 0 bytes free; 22.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     42\u001b[0m     image_files, captions \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 44\u001b[0m     loss \u001b[39m=\u001b[39m forward(model, mask_generator, tokenizer, cfg, param, image_files, captions)\n\u001b[1;32m     45\u001b[0m     eval_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, mask_generator, tokenizer, cfg, param, image_files, captions, prefixs, half)\u001b[0m\n\u001b[1;32m    128\u001b[0m             crops\u001b[39m.\u001b[39mhalf()\n\u001b[1;32m    129\u001b[0m             batch_img_crops\u001b[39m.\u001b[39mappend(crops)\n\u001b[0;32m--> 131\u001b[0m     loss_dict \u001b[39m=\u001b[39m model(data, batch_img_crops) \u001b[39m# hard-coded to cuda\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/decoder.py:81\u001b[0m, in \u001b[0;36mSAMGitCaptioningModel.forward\u001b[0;34m(self, batch, batch_img_crops)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_img_crops):\n\u001b[0;32m---> 81\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_one(batch, batch_img_crops, return_info\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/decoder.py:117\u001b[0m, in \u001b[0;36mSAMGitCaptioningModel.forward_one\u001b[0;34m(self, batch, batch_img_crops, return_info)\u001b[0m\n\u001b[1;32m    114\u001b[0m     visual_features_valid \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(all_valid, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscst):\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_one_ce(batch, visual_features, visual_features_valid, return_info)\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscst\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/decoder.py:217\u001b[0m, in \u001b[0;36mSAMGitCaptioningModel.forward_one_ce\u001b[0;34m(self, batch, visual_features, visual_features_valid, return_info)\u001b[0m\n\u001b[1;32m    215\u001b[0m         output_dict[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feat\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     output_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(batch, visual_features, visual_features_valid)\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/decoder.py:244\u001b[0m, in \u001b[0;36mSAMGitCaptioningModel.infer\u001b[0;34m(self, batch, visual_features, visual_features_valid, search_param)\u001b[0m\n\u001b[1;32m    242\u001b[0m search_param \u001b[39m=\u001b[39m search_param \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    243\u001b[0m \u001b[39m# the start_predictions are not in predicted_caption\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m predicted_caption, logprobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49msearch(\n\u001b[1;32m    245\u001b[0m     start_predictions, decoding_step, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msearch_param\n\u001b[1;32m    246\u001b[0m )\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mprefix\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# we need to remove prefix from predicted_caption\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     predicted_caption \u001b[39m=\u001b[39m predicted_caption[:, start_predictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/decoder.py:1129\u001b[0m, in \u001b[0;36mGeneratorWithBeamSearch.search\u001b[0;34m(self, input_ids, step, num_keep_best, do_sample, top_k, top_p, num_return_sequences)\u001b[0m\n\u001b[1;32m   1126\u001b[0m done \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)]\n\u001b[1;32m   1128\u001b[0m \u001b[39mwhile\u001b[39;00m cur_len \u001b[39m<\u001b[39m max_length:\n\u001b[0;32m-> 1129\u001b[0m     scores \u001b[39m=\u001b[39m step(input_ids)  \u001b[39m# (batch_size * num_beams, cur_len, vocab_size)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     vocab_size \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1132\u001b[0m     \u001b[39m## if model has past, then set the past variable to speed up decoding\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[39m#if self._do_output_past(outputs):\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[39m#past = outputs[1]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m     \u001b[39m# repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/decoder.py:280\u001b[0m, in \u001b[0;36mSAMGitCaptioningModel.decoding_step\u001b[0;34m(self, visual_features, visual_features_valid, bi_valid_mask_caption, partial_captions)\u001b[0m\n\u001b[1;32m    277\u001b[0m     partial_captions \u001b[39m=\u001b[39m partial_captions\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[39m# shape: (batch_size * beam_size, partial_caption_length, vocab_size)\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtextual(\n\u001b[1;32m    281\u001b[0m     visual_features,\n\u001b[1;32m    282\u001b[0m     partial_captions,\n\u001b[1;32m    283\u001b[0m     caption_lengths\u001b[39m=\u001b[39;49mcaption_lengths,\n\u001b[1;32m    284\u001b[0m     hidden_valid_mask\u001b[39m=\u001b[39;49mvisual_features_valid,\n\u001b[1;32m    285\u001b[0m     bi_valid_mask_caption\u001b[39m=\u001b[39;49mbi_valid_mask_caption,\n\u001b[1;32m    286\u001b[0m     encoder_history_states\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprev_encoded_layers,\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscst \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_history_for_infer:\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(logits, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(logits) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/decoder.py:566\u001b[0m, in \u001b[0;36mTransformerDecoderTextualHead.forward\u001b[0;34m(self, hidden_states, caption_tokens, hidden_valid_mask, caption_lengths, bi_valid_mask_caption, encoder_history_states, return_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     extra_param[\u001b[39m'\u001b[39m\u001b[39mencoder_history_states\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m encoder_history_states\n\u001b[1;32m    564\u001b[0m \u001b[39m# if transformer here is the pytorch/decoder, there is no chance, the\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m# output is always tensor\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m trans_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    567\u001b[0m     caption_embeddings,\n\u001b[1;32m    568\u001b[0m     projected_visual_features,\n\u001b[1;32m    569\u001b[0m     memory_key_padding_mask\u001b[39m=\u001b[39;49m(hidden_valid_mask\u001b[39m.\u001b[39;49mlogical_not() \u001b[39mif\u001b[39;49;00m hidden_valid_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    570\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49muni_mask_zero_neg,\n\u001b[1;32m    571\u001b[0m     \u001b[39m#tgt_key_padding_mask=caption_mask,\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m     \u001b[39m#encoder_history_states=encoder_history_states,\u001b[39;49;00m\n\u001b[1;32m    573\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_param,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(trans_out, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    576\u001b[0m     textual_features \u001b[39m=\u001b[39m trans_out[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/decoder.py:152\u001b[0m, in \u001b[0;36mBertEncoderAsDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_bi_valid_mask, encoder_history_states)\u001b[0m\n\u001b[1;32m    149\u001b[0m full_attention_mask \u001b[39m=\u001b[39m full_attention_mask[:, \u001b[39mNone\u001b[39;00m, :, :]\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m encoder_history_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    153\u001b[0m         hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    154\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mfull_attention_mask,\n\u001b[1;32m    155\u001b[0m         encoder_history_states\u001b[39m=\u001b[39;49mencoder_history_states,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    157\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(result)\n\u001b[1;32m    158\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][:, num_memory:]\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/bert/modeling_bert.py:318\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_history_states)\u001b[0m\n\u001b[1;32m    315\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m    317\u001b[0m history_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m encoder_history_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m encoder_history_states[i]\n\u001b[0;32m--> 318\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    319\u001b[0m     hidden_states, attention_mask,\n\u001b[1;32m    320\u001b[0m     (\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m head_mask[i]),\n\u001b[1;32m    321\u001b[0m     history_state,\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/bert/modeling_bert.py:292\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, history_state)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_norm:\n\u001b[0;32m--> 292\u001b[0m         intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    293\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(attention_output))\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/bert/modeling_bert.py:230\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    229\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 230\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/repos/GenerativeImage2Text/generativeimage2text/layers/bert/activations.py:23\u001b[0m, in \u001b[0;36m_gelu_python\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gelu_python\u001b[39m(x):\n\u001b[1;32m     17\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m    Original Implementation of the gelu activation function in Google Bert repo when initially created. For\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    information: OpenAI GPT's gelu is slightly different (and gives slightly different results): 0.5 * x * (1 +\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    torch.nn.functional Also see https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39;49merf(x \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39m2.0\u001b[39;49m)))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 24.00 GiB total capacity; 20.59 GiB already allocated; 0 bytes free; 22.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_losses = []\n",
    "\n",
    "pbar = tqdm(train_loader)\n",
    "for batch in pbar:\n",
    "    if pbar.n < SKIP_STEPS:\n",
    "        time.sleep(0.01)\n",
    "        continue\n",
    "    image_files, captions = batch\n",
    "    \n",
    "    logging.info(f\"memory_allocated: {torch.cuda.memory_allocated()/(1024**2)} MB, max_memory_allocated: {torch.cuda.max_memory_allocated()/(1024**2)} MB\")\n",
    "    loss_dict = forward(model, mask_generator, tokenizer, cfg, param, image_files, captions, half=True)\n",
    "    loss = sum(loss_dict.values())\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_losses.append(loss.detach().cpu().numpy())\n",
    "    logging.info(f\"[step {pbar.n + 1}] loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "    pbar.set_description(f\"[step {pbar.n + 1}] loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "    if (pbar.n + 1) % SAVE_STEPS == 0:\n",
    "        \n",
    "        os.makedirs(f\"output/{MODEL_NAME}/epoch{1}-step{pbar.n+1}\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'losses': train_losses,\n",
    "            }, f\"output/{MODEL_NAME}/epoch{1}-step{pbar.n+1}/model.pt\")\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "# save model dict and loss\n",
    "\n",
    "os.makedirs(f\"output/{MODEL_NAME}/epoch{1}\", exist_ok=True)\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'losses': train_losses,\n",
    "            }, f\"output/{MODEL_NAME}/epoch{1}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev-sd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
