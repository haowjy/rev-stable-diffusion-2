{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '/home/jimmyyao/repos/rev-stable-diffusion-2/notebooks',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python39.zip',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/jimmyyao/.local/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/repos/segment-anything',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/certifi-2022.12.7-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/wheel-0.40.0-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/setuptools-67.6.1-py3.9.egg',\n",
       " '/home/jimmyyao/repos/GenerativeImage2Text']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add to path\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.insert(0, '../')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {}\n",
    "\n",
    "# image_encoder = get_image_encoder(\n",
    "#         param.get('image_encoder_type', 'CLIPViT_B_16'),\n",
    "#         input_resolution=param.get('test_crop_size', 224),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = torch.randn(2, 3, 120, 120)\n",
    "\n",
    "# image_encoder(i1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.rand(768)\n",
    "# t2 = torch.rand(768)\n",
    "# t3 = torch.rand(768)\n",
    "\n",
    "# li = [t1, t2, t3]\n",
    "# # convert li to tensor\n",
    "# t = torch.stack(li)\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'generativeimage2text.preprocess_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m transform\n\u001b[1;32m    147\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgenerativeimage2text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocess_image\u001b[39;00m \u001b[39mimport\u001b[39;00m load_image_by_pil\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_backward\u001b[39m(image_files, captions, prefixs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m prefixs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'generativeimage2text.preprocess_image'"
     ]
    }
   ],
   "source": [
    "from generativeimage2text.train import get_image_transform, get_transform_image_norm, get_inception_train_transform, get_data, collate_fn, recursive_to_device\n",
    "from generativeimage2text.common import Config\n",
    "\n",
    "from generativeimage2text.model import get_image_encoder\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from generativeimage2text.layers.decoder import (TransformerDecoderTextualHead,\n",
    "                             AutoRegressiveBeamSearch, GeneratorWithBeamSearch, create_projecton_layer)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from torch.cuda.amp import autocast\n",
    "    \n",
    "from samgit.decoder import SAMGitCaptioningModel\n",
    "import cv2\n",
    "\n",
    "def sam_image_crops(image, mask_generator, top_n_bbox):\n",
    "    masks = mask_generator.generate(image)\n",
    "    sorted_anns = sorted(masks, key=(lambda x: x['predicted_iou']), reverse=True)[:top_n_bbox]\n",
    "    \n",
    "    crops = []\n",
    "    for ann in sorted_anns:\n",
    "        x0, y0, x1, y1 = ann['bbox']\n",
    "        if x0 > x1: \n",
    "            x0, x1 = x1, x0\n",
    "        if y0 > y1: \n",
    "            y0, y1 = y1, y0\n",
    "\n",
    "        crops.append(image[y0:y1, x0:x1, :])\n",
    "    return crops\n",
    "\n",
    "class ImgObjEncoder(nn.Module):\n",
    "    def __init__(self, image_encoder, top_n_bbox=4, visual_feature_size=768, textual_feature_size=768, visual_projection_type='linearLn'):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.top_n_bbox = top_n_bbox\n",
    "        self.object_projection = create_projecton_layer(\n",
    "            visual_projection_type='linearLn',\n",
    "            visual_feature_size=visual_feature_size + visual_feature_size*top_n_bbox,\n",
    "            textual_feature_size=textual_feature_size,\n",
    "        )\n",
    "        self.textual_feature_size = textual_feature_size\n",
    "    \n",
    "    def forward(self, batch, batch_image_crops: list[torch.Tensor]):\n",
    "        # batch_image_crops = [batch, top_n_bbox, 3, 14, 14]\n",
    "        batch_size = len(batch)\n",
    "        image_features = self.image_encoder(batch)\n",
    "        \n",
    "        for b in range(len(batch_image_crops)): # batch\n",
    "            batch_image_crop_features = self.image_encoder(batch_image_crops[b])\n",
    "            top_n = len(batch_image_crops[b])\n",
    "            print(\"batch_image_crop_features\", batch_image_crop_features.shape)\n",
    "            if top_n < self.top_n_bbox:\n",
    "                padding = torch.zeros(self.top_n_bbox - top_n, self.object_feature_size)\n",
    "                batch_image_crop_features = torch.cat([batch_image_crop_features, padding], dim=0)\n",
    "            crops = []\n",
    "            for i in range(len(image_crops[b])):\n",
    "                crop = image_crops[b][i]\n",
    "                if self.image_transform is not None:\n",
    "                    crop = self.image_transform(crop)\n",
    "                crops.append(crop)\n",
    "            crops = torch.stack(crops)\n",
    "            self.image_transform(crops)\n",
    "            print(\"crops\", crops.shape)\n",
    "            \n",
    "            object_features = self.object_projection(self.image_encoder(crop))\n",
    "            batch_object_features.append(object_features)\n",
    "        \n",
    "        # list ot tensor\n",
    "        padding = torch.stack(batch_padding, dim=0)\n",
    "        object_features = torch.stack(batch_object_features, dim=0)\n",
    "        # concat\n",
    "        \n",
    "        features = torch.cat([padding, image_features, object_features], dim=1)\n",
    "        return features\n",
    "\n",
    "\n",
    "def get_samgit_model(tokenizer, param):\n",
    "    image_encoder = get_image_encoder(\n",
    "        param.get('image_encoder_type', 'CLIPViT_B_16'),\n",
    "        input_resolution=param.get('test_crop_size', 224),\n",
    "    )\n",
    "    \n",
    "    img_obj_encoder = ImgObjEncoder(\n",
    "        image_encoder,\n",
    "        param.get('top_n_bbox', 4),\n",
    "        visual_feature_size=param.get('visual_feature_size', 768),\n",
    "        textual_feature_size=768,\n",
    "        visual_projection_type=param.get('visual_projection_type', 'linearLn'),\n",
    "    )\n",
    "    text_decoder = TransformerDecoderTextualHead(\n",
    "        visual_feature_size=param.get('visual_feature_size', 768),\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_layers=6,\n",
    "        attention_heads=12,\n",
    "        feedforward_size=768* 4,\n",
    "        max_caption_length=1024,\n",
    "        mask_future_positions=True,\n",
    "        padding_idx=0,\n",
    "        decoder_type='bert_en',\n",
    "        visual_projection_type='linearLn',\n",
    "    )\n",
    "    decoder = GeneratorWithBeamSearch(\n",
    "        eos_index=tokenizer.sep_token_id,\n",
    "        #max_steps=40,\n",
    "        max_steps=1024,\n",
    "        beam_size=4,\n",
    "        length_penalty=0.6,\n",
    "    )\n",
    "    \n",
    "    model = SAMGitCaptioningModel(\n",
    "        img_obj_encoder,\n",
    "        text_decoder,\n",
    "        decoder=decoder,\n",
    "        sos_index=tokenizer.cls_token_id,\n",
    "        eos_index=tokenizer.sep_token_id,\n",
    "        tokenizer=tokenizer,\n",
    "        use_history_for_infer=True,\n",
    "        loss_type='smooth',\n",
    "        num_image_with_embedding=param.get('num_image_with_embedding')\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_small_transform_vit_default(cfg, crop=14):\n",
    "    default_normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    normalize = get_transform_image_norm(cfg, default_normalize)\n",
    "    transform = get_inception_train_transform(\n",
    "        bgr2rgb=True,\n",
    "        crop_size=crop,\n",
    "        normalize=normalize,\n",
    "        small_scale=cfg.input_small_scale,\n",
    "        no_color_jitter=cfg.no_color_jitter,\n",
    "        no_flip=cfg.no_flip,\n",
    "        no_aspect_dist=cfg.no_aspect_dist,\n",
    "        resize_crop=cfg.resize_crop,\n",
    "        max_size=cfg.train_max_size,\n",
    "        interpolation=cfg.interpolation or Image.BILINEAR,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from generativeimage2text.process_image import load_image_by_pil\n",
    "def forward_backward(image_files, captions, prefixs=None):\n",
    "    if prefixs is None:\n",
    "        prefixs = [''] * len(captions)\n",
    "    cfg = {\n",
    "        'crop_region_extend_in_datatransform': 4,\n",
    "        'data_normalize': 'clip',\n",
    "        'train_crop_size': 224,\n",
    "        'input_small_scale': 0.8,\n",
    "        'no_color_jitter': True,\n",
    "        'no_flip': True,\n",
    "        'no_aspect_dist': True,\n",
    "        'interpolation': 'bicubic',\n",
    "        'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        # 'min_size_range32': [224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        'patch_size': 16,\n",
    "        'train_transform': 'vitp',\n",
    "    }\n",
    "    cfg = Config(cfg, {})\n",
    "    all_data = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    image_transform = get_image_transform(cfg)\n",
    "    for image_file, prefix, target in zip(image_files, prefixs, captions):\n",
    "        data = get_data(image_file, prefix, target,\n",
    "                        tokenizer, image_transform)\n",
    "        all_data.append(data)\n",
    "    data = collate_fn(all_data)\n",
    "    data = recursive_to_device(data, 'cuda')\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    model = get_samgit_model(tokenizer, param)\n",
    "    model.half()\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    sam = sam_model_registry[param.get(\"sam_model_type\", \"vit_h\")](checkpoint=param.get(\"sam_checkpoint\", \"../models/sam_vit_h_4b8939.pth\")).half()\n",
    "    sam.half()\n",
    "    sam.eval()\n",
    "    sam.requires_grad_(False)\n",
    "    sam.to(device='cuda')\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=4,\n",
    "        pred_iou_thresh=0.95,\n",
    "        stability_score_thresh=0.95,\n",
    "    )\n",
    "    \n",
    "    # resize to 14x14\n",
    "    small_transform = get_small_transform_vit_default(cfg, crop=14)\n",
    "    \n",
    "    print(\"cuda util:\",torch.cuda.utilization())\n",
    "    with autocast():\n",
    "        batch_img_crops = []\n",
    "        for img_name in image_files:\n",
    "            image = load_image_by_pil(img_name)\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            crops = sam_image_crops(image, mask_generator, param.get('top_n_bbox', 4))\n",
    "            \n",
    "            print(crops[0].shape)\n",
    "            print(type(crops[0]))\n",
    "            crops = small_transform(crops)\n",
    "            crops = torch.stack(crops)\n",
    "            batch_img_crops.append(crops)\n",
    "            \n",
    "        loss_dict = model(data, batch_img_crops)\n",
    "        \n",
    "    loss = sum(loss_dict.values())\n",
    "    loss.backward()\n",
    "    logging.info(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20057f34d</td>\n",
       "      <td>hyper realistic photo of very friendly and dys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227ef0887</td>\n",
       "      <td>ramen carved out of fractal rose ebony, in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e911621</td>\n",
       "      <td>ultrasaurus holding a black bean taco in the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4e1c55a9</td>\n",
       "      <td>a thundering retro robot crane inks on parchme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c98f79f71</td>\n",
       "      <td>portrait painting of a shimmering greek hero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d8edf2e40</td>\n",
       "      <td>an astronaut standing on a engaging white rose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f27825b2c</td>\n",
       "      <td>Kaggle employee Phil at a donut shop ordering ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imgId                                             prompt\n",
       "0  20057f34d  hyper realistic photo of very friendly and dys...\n",
       "1  227ef0887  ramen carved out of fractal rose ebony, in the...\n",
       "2  92e911621  ultrasaurus holding a black bean taco in the w...\n",
       "3  a4e1c55a9  a thundering retro robot crane inks on parchme...\n",
       "4  c98f79f71  portrait painting of a shimmering greek hero, ...\n",
       "5  d8edf2e40  an astronaut standing on a engaging white rose...\n",
       "6  f27825b2c  Kaggle employee Phil at a donut shop ordering ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prompt_df = pd.read_csv('../datasets/sampleeval/prompts.csv')\n",
    "prompt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../datasets/sampleeval/20057f34d.png', '../datasets/sampleeval/227ef0887.png', '../datasets/sampleeval/92e911621.png', '../datasets/sampleeval/a4e1c55a9.png', '../datasets/sampleeval/c98f79f71.png', '../datasets/sampleeval/d8edf2e40.png', '../datasets/sampleeval/f27825b2c.png']\n",
      "['hyper realistic photo of very friendly and dystopian crater', 'ramen carved out of fractal rose ebony, in the style of hudson river school', 'ultrasaurus holding a black bean taco in the woods, near an identical cheneosaurus', 'a thundering retro robot crane inks on parchment with a droopy french bulldog', 'portrait painting of a shimmering greek hero, next to a loud frill-necked lizard', 'an astronaut standing on a engaging white rose, in the midst of by ivory cherry blossoms', 'Kaggle employee Phil at a donut shop ordering all the best donuts, with a speech bubble that proclaims \"Donuts. It\\'s what\\'s for dinner!\"']\n"
     ]
    }
   ],
   "source": [
    "image_files = [f\"../datasets/sampleeval/{img}.png\" for img in prompt_df['imgId']]\n",
    "captions = [cap for cap in prompt_df['prompt']]\n",
    "\n",
    "print(image_files)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "forward_backward(image_files, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev-sd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
