{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '/home/jimmyyao/repos/rev-stable-diffusion-2/notebooks',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python39.zip',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/jimmyyao/.local/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/repos/segment-anything',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/certifi-2022.12.7-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/wheel-0.40.0-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/setuptools-67.6.1-py3.9.egg',\n",
       " '/home/jimmyyao/repos/GenerativeImage2Text']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add to path\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.insert(0, '../')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {}\n",
    "\n",
    "# image_encoder = get_image_encoder(\n",
    "#         param.get('image_encoder_type', 'CLIPViT_B_16'),\n",
    "#         input_resolution=param.get('test_crop_size', 224),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = torch.randn(2, 3, 120, 120)\n",
    "\n",
    "# image_encoder(i1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.rand(768)\n",
    "# t2 = torch.rand(768)\n",
    "# t3 = torch.rand(768)\n",
    "\n",
    "# li = [t1, t2, t3]\n",
    "# # convert li to tensor\n",
    "# t = torch.stack(li)\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from generativeimage2text.train import (get_image_transform, get_transform_image_norm, \n",
    "                                        get_inception_train_transform, \n",
    "                                        get_data, collate_fn, recursive_to_device)\n",
    "from generativeimage2text.common import Config\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from torch.cuda.amp import autocast\n",
    "    \n",
    "from samgit.model import sam_image_crops, get_samgit_model\n",
    "\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_small_transform_vit_default(cfg, crop=24):\n",
    "    default_normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    normalize = get_transform_image_norm(cfg, default_normalize)\n",
    "    transform = get_inception_train_transform(\n",
    "        bgr2rgb=True,\n",
    "        crop_size=crop,\n",
    "        normalize=normalize,\n",
    "        small_scale=cfg.input_small_scale,\n",
    "        no_color_jitter=cfg.no_color_jitter,\n",
    "        no_flip=cfg.no_flip,\n",
    "        no_aspect_dist=cfg.no_aspect_dist,\n",
    "        resize_crop=cfg.resize_crop,\n",
    "        max_size=cfg.train_max_size,\n",
    "        interpolation=cfg.interpolation or Image.BILINEAR,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "def prepare_model():\n",
    "    cfg = {\n",
    "        'crop_region_extend_in_datatransform': 4,\n",
    "        'data_normalize': 'clip',\n",
    "        'train_crop_size': 224,\n",
    "        'input_small_scale': 0.8,\n",
    "        'no_color_jitter': True,\n",
    "        'no_flip': True,\n",
    "        'no_aspect_dist': True,\n",
    "        'interpolation': 'bicubic',\n",
    "        'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        # 'min_size_range32': [224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        'patch_size': 16,\n",
    "        'train_transform': 'vitp',\n",
    "    }\n",
    "    cfg = Config(cfg, {})\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    model = get_samgit_model(tokenizer, param)\n",
    "    model.half()\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    \n",
    "    # preprocess object crops with sam\n",
    "    sam = sam_model_registry[param.get(\"sam_model_type\", \"vit_h\")](checkpoint=param.get(\"sam_checkpoint\", \"../models/sam_vit_h_4b8939.pth\")).half()\n",
    "    sam.half()\n",
    "    sam.eval()\n",
    "    sam.requires_grad_(False)\n",
    "    sam.to(device='cuda')\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=4,\n",
    "        pred_iou_thresh=0.95,\n",
    "        stability_score_thresh=0.95,\n",
    "    )\n",
    "    \n",
    "    return model, mask_generator, cfg, param\n",
    "\n",
    "def forward_backward(model, mask_generator, cfg, param, image_files, captions, prefixs=None):\n",
    "    if prefixs is None:\n",
    "        prefixs = [''] * len(captions)\n",
    "\n",
    "    all_data = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    image_transform = get_image_transform(cfg)\n",
    "    # resize to 24x24\n",
    "    small_transform = get_small_transform_vit_default(cfg, crop=24)\n",
    "    \n",
    "    for image_file, prefix, target in zip(image_files, prefixs, captions):\n",
    "        data = get_data(image_file, prefix, target,\n",
    "                        tokenizer, image_transform)\n",
    "        all_data.append(data)\n",
    "    data = collate_fn(all_data)\n",
    "    data = recursive_to_device(data, 'cuda')\n",
    "    \n",
    "    logging.info(\"cuda util: %s\", torch.cuda.utilization())\n",
    "    with autocast():\n",
    "        batch_img_crops = []\n",
    "        for img_name in image_files:\n",
    "            image = cv2.imread(img_name)\n",
    "            crops = sam_image_crops(image, mask_generator, param.get('top_n_bbox', 4))\n",
    "            new_crops = []\n",
    "            for crop in crops:\n",
    "                if 0 in crop.shape:\n",
    "                    continue\n",
    "                crop = Image.fromarray(crop)\n",
    "                new_crops.append(small_transform(crop))\n",
    "\n",
    "            if len(new_crops) == 0:\n",
    "                batch_img_crops.append(None)\n",
    "            else:\n",
    "                crops = torch.stack(new_crops)\n",
    "                crops.half()\n",
    "                batch_img_crops.append(crops)\n",
    "            \n",
    "        loss_dict = model(data, batch_img_crops) # hard-coded to cuda\n",
    "        \n",
    "    loss = sum(loss_dict.values())\n",
    "    loss.backward()\n",
    "    logging.info(loss)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>part</th>\n",
       "      <th>new_image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0301-to-0400...</td>\n",
       "      <td>bbc article announcing the beginning of a nucl...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>haunted house ghost bugs dark room grey black</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0601-to-0700...</td>\n",
       "      <td>la llorona serving cunt and being fierce</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0501-to-0600...</td>\n",
       "      <td>Medusa, fantasy horror art, hissing scream, fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/c67d39f8-98f4-439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0101-to-0200...</td>\n",
       "      <td>cleopatra in ancient egypt in front of a pyram...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b5141b36-447c-492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144890</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0701-to-0800...</td>\n",
       "      <td>racionais mc's com traje de astronauta</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/546518b9-74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144891</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0801-to-0900...</td>\n",
       "      <td>dante traveling through the 7 layers of hell</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/02ceb3f5-30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144892</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1801-to-1900...</td>\n",
       "      <td>Spike Spiegel by Kentaro Miura, Charachter Por...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/21f3563b-6d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144893</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>can't sleep. clown will eat me. horror ; night...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/8f13247b-55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144894</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0201-to-0300...</td>\n",
       "      <td>photo of macintosh 1 2 8 k with mouse and keyb...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/3c026626-1a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144895 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name   \n",
       "0       /kaggle/input/diffusiondb-2m-part-0301-to-0400...  \\\n",
       "1       /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "2       /kaggle/input/diffusiondb-2m-part-0601-to-0700...   \n",
       "3       /kaggle/input/diffusiondb-2m-part-0501-to-0600...   \n",
       "4       /kaggle/input/diffusiondb-2m-part-0101-to-0200...   \n",
       "...                                                   ...   \n",
       "144890  /kaggle/input/diffusiondb-2m-part-0701-to-0800...   \n",
       "144891  /kaggle/input/diffusiondb-2m-part-0801-to-0900...   \n",
       "144892  /kaggle/input/diffusiondb-2m-part-1801-to-1900...   \n",
       "144893  /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "144894  /kaggle/input/diffusiondb-2m-part-0201-to-0300...   \n",
       "\n",
       "                                                   prompt  part   \n",
       "0       bbc article announcing the beginning of a nucl...     0  \\\n",
       "1           haunted house ghost bugs dark room grey black     0   \n",
       "2                la llorona serving cunt and being fierce     0   \n",
       "3       Medusa, fantasy horror art, hissing scream, fa...     0   \n",
       "4       cleopatra in ancient egypt in front of a pyram...     0   \n",
       "...                                                   ...   ...   \n",
       "144890             racionais mc's com traje de astronauta     3   \n",
       "144891       dante traveling through the 7 layers of hell     3   \n",
       "144892  Spike Spiegel by Kentaro Miura, Charachter Por...     3   \n",
       "144893  can't sleep. clown will eat me. horror ; night...     3   \n",
       "144894  photo of macintosh 1 2 8 k with mouse and keyb...     3   \n",
       "\n",
       "                                           new_image_name  \n",
       "0       diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...  \n",
       "1       diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...  \n",
       "2       diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...  \n",
       "3       diffusiondb-filtered-0-40000/c67d39f8-98f4-439...  \n",
       "4       diffusiondb-filtered-0-40000/b5141b36-447c-492...  \n",
       "...                                                   ...  \n",
       "144890  diffusiondb-filtered-120001-144894/546518b9-74...  \n",
       "144891  diffusiondb-filtered-120001-144894/02ceb3f5-30...  \n",
       "144892  diffusiondb-filtered-120001-144894/21f3563b-6d...  \n",
       "144893  diffusiondb-filtered-120001-144894/8f13247b-55...  \n",
       "144894  diffusiondb-filtered-120001-144894/3c026626-1a...  \n",
       "\n",
       "[144895 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "import os\n",
    "import pandas as pd\n",
    "DIFFUSIONDB_IMGS_PATH = \"/mnt/d/AiStuff/data/diffusionDB2M-f2/images\"\n",
    "\n",
    "# # csvs\n",
    "# csvs = [\"diffusiondb-filtered-0-40000.csv\", \"diffusiondb-filtered-40001-80000.csv\", \"diffusiondb-filtered-80001-120000.csv\", \"diffusiondb-filtered-120001-144894.csv\"]\n",
    "# dfs = []\n",
    "# for csv in csvs:\n",
    "#     df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, csv))\n",
    "#     dfs.append(df)\n",
    "# df = pd.concat(dfs)\n",
    "\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered-0-40000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 100 for eval\n",
    "df_eval = df[:100]\n",
    "# rest for training\n",
    "df_train = df[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 350837078/350837078 [00:06<00:00, 52104326.38it/s]\n",
      "[5] loss: nan:   0%|          | 6/9056 [00:31<13:22:54,  5.32s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m image_files \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DIFFUSIONDB_IMGS_PATH, image_file) \u001b[39mfor\u001b[39;00m image_file \u001b[39min\u001b[39;00m image_files]\n\u001b[1;32m     18\u001b[0m captions \u001b[39m=\u001b[39m chunk[\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 20\u001b[0m loss \u001b[39m=\u001b[39m forward_backward(model, mask_generator, cfg, param, image_files, captions)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m, in \u001b[0;36mforward_backward\u001b[0;34m(model, mask_generator, cfg, param, image_files, captions, prefixs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m img_name \u001b[39min\u001b[39;00m image_files:\n\u001b[1;32m    106\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_name)\n\u001b[0;32m--> 107\u001b[0m     crops \u001b[39m=\u001b[39m sam_image_crops(image, mask_generator, param\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtop_n_bbox\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m4\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     new_crops \u001b[39m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m     \u001b[39mfor\u001b[39;00m crop \u001b[39min\u001b[39;00m crops:\n",
      "File \u001b[0;32m~/repos/rev-stable-diffusion-2/notebooks/../samgit/model.py:13\u001b[0m, in \u001b[0;36msam_image_crops\u001b[0;34m(image, mask_generator, top_n_bbox)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msam_image_crops\u001b[39m(image, mask_generator, top_n_bbox):\n\u001b[0;32m---> 13\u001b[0m     masks \u001b[39m=\u001b[39m mask_generator\u001b[39m.\u001b[39;49mgenerate(image)\n\u001b[1;32m     14\u001b[0m     sorted_anns \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(masks, key\u001b[39m=\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mpredicted_iou\u001b[39m\u001b[39m'\u001b[39m]), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:top_n_bbox]\n\u001b[1;32m     16\u001b[0m     crops \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_masks(image)\n\u001b[1;32m    165\u001b[0m \u001b[39m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_mask_region_area \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[39m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m crop_box, layer_idx \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_crop(image, crop_box, layer_idx, orig_size)\n\u001b[1;32m    207\u001b[0m     data\u001b[39m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/automatic_mask_generator.py:236\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    234\u001b[0m cropped_im \u001b[39m=\u001b[39m image[y0:y1, x0:x1, :]\n\u001b[1;32m    235\u001b[0m cropped_im_size \u001b[39m=\u001b[39m cropped_im\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 236\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mset_image(cropped_im)\n\u001b[1;32m    238\u001b[0m \u001b[39m# Get points for this crop\u001b[39;00m\n\u001b[1;32m    239\u001b[0m points_scale \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(cropped_im_size)[\u001b[39mNone\u001b[39;00m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(input_image, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[39m=\u001b[39m input_image_torch\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_torch_image(input_image_torch, image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_image_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[39m=\u001b[39m window_partition(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)\n\u001b[1;32m    175\u001b[0m \u001b[39m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale) \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[39m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    236\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mreshape(B, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[39m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[39m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[39m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[1;32m    350\u001b[0m Rw \u001b[39m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/repos/segment-anything/segment_anything/modeling/image_encoder.py:322\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    319\u001b[0m k_coords \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(k_size)[\u001b[39mNone\u001b[39;00m, :] \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    320\u001b[0m relative_coords \u001b[39m=\u001b[39m (q_coords \u001b[39m-\u001b[39m k_coords) \u001b[39m+\u001b[39m (k_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m--> 322\u001b[0m \u001b[39mreturn\u001b[39;00m rel_pos_resized[relative_coords\u001b[39m.\u001b[39;49mlong()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1 epoch\n",
    "model, mask_generator, cfg, param = prepare_model()\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "pbar = tqdm(np.array_split(df_train, (len(df)//16) + 1))\n",
    "for chunk in pbar:\n",
    "    image_files = chunk['new_image_name'].tolist()\n",
    "    image_files = [os.path.join(DIFFUSIONDB_IMGS_PATH, image_file) for image_file in image_files]\n",
    "    captions = chunk['prompt'].tolist()\n",
    "    \n",
    "    loss = forward_backward(model, mask_generator, cfg, param, image_files, captions)\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar.set_description(f\"[{pbar.n}] loss: {loss:.4f}\")\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"EVAL\")\n",
    "model.eval()\n",
    "eval_losses = []\n",
    "for i, chunk in enumerate(np.array_split(df_eval, (len(df)//16) + 1)):\n",
    "    \n",
    "    image_files = chunk['new_image_name'].tolist()\n",
    "    captions = chunk['prompt'].tolist()\n",
    "    \n",
    "    loss = forward_backward(model, mask_generator, cfg, param, image_files, captions)\n",
    "    eval_losses.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model dict and loss\n",
    "\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, f\"checkpoint-{1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../datasets/sampleeval/20057f34d.png', '../datasets/sampleeval/227ef0887.png', '../datasets/sampleeval/92e911621.png', '../datasets/sampleeval/a4e1c55a9.png', '../datasets/sampleeval/c98f79f71.png', '../datasets/sampleeval/d8edf2e40.png', '../datasets/sampleeval/f27825b2c.png']\n",
      "['hyper realistic photo of very friendly and dystopian crater', 'ramen carved out of fractal rose ebony, in the style of hudson river school', 'ultrasaurus holding a black bean taco in the woods, near an identical cheneosaurus', 'a thundering retro robot crane inks on parchment with a droopy french bulldog', 'portrait painting of a shimmering greek hero, next to a loud frill-necked lizard', 'an astronaut standing on a engaging white rose, in the midst of by ivory cherry blossoms', 'Kaggle employee Phil at a donut shop ordering all the best donuts, with a speech bubble that proclaims \"Donuts. It\\'s what\\'s for dinner!\"']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prompt_df = pd.read_csv('../datasets/sampleeval/prompts.csv')\n",
    "image_files = [f\"../datasets/sampleeval/{img}.png\" for img in prompt_df['imgId']]\n",
    "captions = [cap for cap in prompt_df['prompt']]\n",
    "\n",
    "print(image_files)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda util: 94\n",
      "image_features torch.Size([7, 101, 768])\n"
     ]
    }
   ],
   "source": [
    "# forward_backward(image_files, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev-sd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
