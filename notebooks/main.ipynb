{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '/home/jimmyyao/repos/rev-stable-diffusion-2/notebooks',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python39.zip',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/jimmyyao/.local/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages',\n",
       " '/home/jimmyyao/repos/segment-anything',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/certifi-2022.12.7-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/wheel-0.40.0-py3.9.egg',\n",
       " '/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/setuptools-67.6.1-py3.9.egg',\n",
       " '/home/jimmyyao/repos/GenerativeImage2Text']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add to path\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.insert(0, '../')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {}\n",
    "\n",
    "# image_encoder = get_image_encoder(\n",
    "#         param.get('image_encoder_type', 'CLIPViT_B_16'),\n",
    "#         input_resolution=param.get('test_crop_size', 224),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = torch.randn(2, 3, 120, 120)\n",
    "\n",
    "# image_encoder(i1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.rand(768)\n",
    "# t2 = torch.rand(768)\n",
    "# t3 = torch.rand(768)\n",
    "\n",
    "# li = [t1, t2, t3]\n",
    "# # convert li to tensor\n",
    "# t = torch.stack(li)\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    \n",
    "    return info\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    info = get_gpu_utilization()\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1425 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/rev-sd2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from generativeimage2text.train import (get_image_transform, get_transform_image_norm, \n",
    "                                        get_inception_train_transform, \n",
    "                                        get_data, collate_fn, recursive_to_device)\n",
    "from generativeimage2text.common import Config\n",
    "from generativeimage2text.torch_common import load_state_dict\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from torch.cuda.amp import autocast\n",
    "    \n",
    "from samgit.model import sam_image_crops, get_samgit_model\n",
    "\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_small_transform_vit_default(cfg, crop=24):\n",
    "    default_normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    normalize = get_transform_image_norm(cfg, default_normalize)\n",
    "    transform = get_inception_train_transform(\n",
    "        bgr2rgb=True,\n",
    "        crop_size=crop,\n",
    "        normalize=normalize,\n",
    "        small_scale=cfg.input_small_scale,\n",
    "        no_color_jitter=cfg.no_color_jitter,\n",
    "        no_flip=cfg.no_flip,\n",
    "        no_aspect_dist=cfg.no_aspect_dist,\n",
    "        resize_crop=cfg.resize_crop,\n",
    "        max_size=cfg.train_max_size,\n",
    "        interpolation=cfg.interpolation or Image.BILINEAR,\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "def prepare_model(pretrained_path=None, half=True):\n",
    "    cfg = {\n",
    "        'crop_region_extend_in_datatransform': 4,\n",
    "        'data_normalize': 'clip',\n",
    "        'train_crop_size': 224,\n",
    "        'input_small_scale': 0.8,\n",
    "        'no_color_jitter': True,\n",
    "        'no_flip': True,\n",
    "        'no_aspect_dist': True,\n",
    "        'interpolation': 'bicubic',\n",
    "        'min_size_range32': [160, 224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        # 'min_size_range32': [224], # in pretraining, it is multi-scale from 160 to 224; while for fine-tuning, it is single scale\n",
    "        'patch_size': 16,\n",
    "        'train_transform': 'vitp',\n",
    "    }\n",
    "    cfg = Config(cfg, {})\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    model = get_samgit_model(tokenizer, param)\n",
    "    \n",
    "    # from pre-trained model\n",
    "    if pretrained_path is not None:\n",
    "        checkpoint = torch.load(pretrained_path)['model']\n",
    "        load_state_dict(model, checkpoint)\n",
    "        \n",
    "    # if half:\n",
    "    #     model.half()\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    param = {\n",
    "        \"top_n_bbox\": 4,\n",
    "    }\n",
    "    \n",
    "    # preprocess object crops with sam\n",
    "    sam = sam_model_registry[param.get(\"sam_model_type\", \"vit_h\")](checkpoint=param.get(\"sam_checkpoint\", \"../models/sam_vit_h_4b8939.pth\")).half()\n",
    "    # if half:\n",
    "    #     sam.half()\n",
    "    sam.eval()\n",
    "    sam.requires_grad_(False)\n",
    "    sam.to(device='cuda')\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=4,\n",
    "        pred_iou_thresh=0.95,\n",
    "        stability_score_thresh=0.95,\n",
    "    )\n",
    "    \n",
    "    return model, mask_generator, tokenizer, cfg, param\n",
    "\n",
    "def forward(model, mask_generator, tokenizer, cfg, param, image_files, captions, prefixs=None, half=True):\n",
    "    if prefixs is None:\n",
    "        prefixs = [''] * len(captions)\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    image_transform = get_image_transform(cfg)\n",
    "    # resize to 24x24\n",
    "    small_transform = get_small_transform_vit_default(cfg, crop=24)\n",
    "    \n",
    "    for image_file, prefix, target in zip(image_files, prefixs, captions):\n",
    "        data = get_data(image_file, prefix, target,\n",
    "                        tokenizer, image_transform)\n",
    "        all_data.append(data)\n",
    "    data = collate_fn(all_data)\n",
    "    data = recursive_to_device(data, 'cuda')\n",
    "    \n",
    "    logging.info(f\"GPU memory occupied: {get_gpu_utilization().used//1024**2} MB.\")\n",
    "    \n",
    "    batch_img_crops = []\n",
    "    with autocast(dtype=torch.float16 if half else torch.float32):\n",
    "        for img_name in image_files:\n",
    "            image = cv2.imread(img_name)\n",
    "            crops = sam_image_crops(image, mask_generator, param.get('top_n_bbox', 4))\n",
    "            new_crops = []\n",
    "            for crop in crops:\n",
    "                if 0 in crop.shape:\n",
    "                    continue\n",
    "                crop = Image.fromarray(crop)\n",
    "                new_crops.append(small_transform(crop))\n",
    "\n",
    "            if len(new_crops) == 0:\n",
    "                batch_img_crops.append(None)\n",
    "            else:\n",
    "                crops = torch.stack(new_crops)\n",
    "                crops.half()\n",
    "                batch_img_crops.append(crops)\n",
    "            \n",
    "        loss_dict = model(data, batch_img_crops) # hard-coded to cuda\n",
    "    return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>part</th>\n",
       "      <th>new_image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0301-to-0400...</td>\n",
       "      <td>bbc article announcing the beginning of a nucl...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>haunted house ghost bugs dark room grey black</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0601-to-0700...</td>\n",
       "      <td>la llorona serving cunt and being fierce</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0501-to-0600...</td>\n",
       "      <td>Medusa, fantasy horror art, hissing scream, fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/c67d39f8-98f4-439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0101-to-0200...</td>\n",
       "      <td>cleopatra in ancient egypt in front of a pyram...</td>\n",
       "      <td>0</td>\n",
       "      <td>diffusiondb-filtered-0-40000/b5141b36-447c-492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144890</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0701-to-0800...</td>\n",
       "      <td>racionais mc's com traje de astronauta</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/546518b9-74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144891</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0801-to-0900...</td>\n",
       "      <td>dante traveling through the 7 layers of hell</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/02ceb3f5-30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144892</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1801-to-1900...</td>\n",
       "      <td>Spike Spiegel by Kentaro Miura, Charachter Por...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/21f3563b-6d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144893</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-1301-to-1400...</td>\n",
       "      <td>can't sleep. clown will eat me. horror ; night...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/8f13247b-55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144894</th>\n",
       "      <td>/kaggle/input/diffusiondb-2m-part-0201-to-0300...</td>\n",
       "      <td>photo of macintosh 1 2 8 k with mouse and keyb...</td>\n",
       "      <td>3</td>\n",
       "      <td>diffusiondb-filtered-120001-144894/3c026626-1a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144895 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name   \n",
       "0       /kaggle/input/diffusiondb-2m-part-0301-to-0400...  \\\n",
       "1       /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "2       /kaggle/input/diffusiondb-2m-part-0601-to-0700...   \n",
       "3       /kaggle/input/diffusiondb-2m-part-0501-to-0600...   \n",
       "4       /kaggle/input/diffusiondb-2m-part-0101-to-0200...   \n",
       "...                                                   ...   \n",
       "144890  /kaggle/input/diffusiondb-2m-part-0701-to-0800...   \n",
       "144891  /kaggle/input/diffusiondb-2m-part-0801-to-0900...   \n",
       "144892  /kaggle/input/diffusiondb-2m-part-1801-to-1900...   \n",
       "144893  /kaggle/input/diffusiondb-2m-part-1301-to-1400...   \n",
       "144894  /kaggle/input/diffusiondb-2m-part-0201-to-0300...   \n",
       "\n",
       "                                                   prompt  part   \n",
       "0       bbc article announcing the beginning of a nucl...     0  \\\n",
       "1           haunted house ghost bugs dark room grey black     0   \n",
       "2                la llorona serving cunt and being fierce     0   \n",
       "3       Medusa, fantasy horror art, hissing scream, fa...     0   \n",
       "4       cleopatra in ancient egypt in front of a pyram...     0   \n",
       "...                                                   ...   ...   \n",
       "144890             racionais mc's com traje de astronauta     3   \n",
       "144891       dante traveling through the 7 layers of hell     3   \n",
       "144892  Spike Spiegel by Kentaro Miura, Charachter Por...     3   \n",
       "144893  can't sleep. clown will eat me. horror ; night...     3   \n",
       "144894  photo of macintosh 1 2 8 k with mouse and keyb...     3   \n",
       "\n",
       "                                           new_image_name  \n",
       "0       diffusiondb-filtered-0-40000/b935f1f4-00fe-47f...  \n",
       "1       diffusiondb-filtered-0-40000/f3c98c88-d89c-42b...  \n",
       "2       diffusiondb-filtered-0-40000/8d4dd42f-846f-4e7...  \n",
       "3       diffusiondb-filtered-0-40000/c67d39f8-98f4-439...  \n",
       "4       diffusiondb-filtered-0-40000/b5141b36-447c-492...  \n",
       "...                                                   ...  \n",
       "144890  diffusiondb-filtered-120001-144894/546518b9-74...  \n",
       "144891  diffusiondb-filtered-120001-144894/02ceb3f5-30...  \n",
       "144892  diffusiondb-filtered-120001-144894/21f3563b-6d...  \n",
       "144893  diffusiondb-filtered-120001-144894/8f13247b-55...  \n",
       "144894  diffusiondb-filtered-120001-144894/3c026626-1a...  \n",
       "\n",
       "[144895 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "import os\n",
    "import pandas as pd\n",
    "DIFFUSIONDB_IMGS_PATH = \"/mnt/d/AiStuff/data/diffusionDB2M-f2/images\"\n",
    "\n",
    "# # csvs\n",
    "# csvs = [\"diffusiondb-filtered-0-40000.csv\", \"diffusiondb-filtered-40001-80000.csv\", \"diffusiondb-filtered-80001-120000.csv\", \"diffusiondb-filtered-120001-144894.csv\"]\n",
    "# dfs = []\n",
    "# for csv in csvs:\n",
    "#     df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, csv))\n",
    "#     dfs.append(df)\n",
    "# df = pd.concat(dfs)\n",
    "\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 100 for eval\n",
    "df_eval = df[:100]\n",
    "# rest for training\n",
    "df_train = df[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "class DiffusionDBDataset(Dataset):\n",
    "    def __init__(self, df, db_base_path):\n",
    "        self.df = df\n",
    "        self.db_base_path = db_base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_file = os.path.join(self.db_base_path, row[\"new_image_name\"])\n",
    "        prompt = row[\"prompt\"]\n",
    "        return image_file, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.path.join(DIFFUSIONDB_IMGS_PATH, \"diffusiondb-filtered-0-40000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='train.info.log', encoding='utf-8', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "WORKERS = 2\n",
    "\n",
    "LR = 5e-5 # https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md#learning-rate-notes 1e-5 to 5e-5... although they suggest a schedule\n",
    "\n",
    "SAVE_STEPS = 200\n",
    "\n",
    "SKIP_STEPS = 700\n",
    "PRETRAINED_PATH = f\"output/SAMGIT/epoch1-step{SKIP_STEPS}/model.pt\"\n",
    "MODEL_NAME = \"SAMGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1 epoch\n",
    "model, mask_generator, tokenizer, cfg, param = prepare_model(pretrained_path=PRETRAINED_PATH, half=True)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiffusionDBDataset(df_train, DIFFUSIONDB_IMGS_PATH)\n",
    "train_random_sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_random_sampler, batch_size=BATCH_SIZE, num_workers=WORKERS)\n",
    "\n",
    "eval_dataset = DiffusionDBDataset(df_eval, DIFFUSIONDB_IMGS_PATH)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 4486 MB.\n",
      "memory_allocated: 1812.8544921875 MB, max_memory_allocated: 2402.02197265625 MB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n",
    "print(f\"memory_allocated: {torch.cuda.memory_allocated()/(1024**2)} MB, max_memory_allocated: {torch.cuda.max_memory_allocated()/(1024**2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[step 711] loss: 3.9892:  63%|██████▎   | 711/1132 [06:54<3:38:11, 31.10s/it]"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_losses = []\n",
    "\n",
    "pbar = tqdm(train_loader)\n",
    "for batch in pbar:\n",
    "    if pbar.n < SKIP_STEPS:\n",
    "        time.sleep(0.01)\n",
    "        continue\n",
    "    image_files, captions = batch\n",
    "    \n",
    "    logging.info(f\"memory_allocated: {torch.cuda.memory_allocated()/(1024**2)} MB, max_memory_allocated: {torch.cuda.max_memory_allocated()/(1024**2)} MB\")\n",
    "    loss_dict = forward(model, mask_generator, tokenizer, cfg, param, image_files, captions, half=True)\n",
    "    loss = sum(loss_dict.values())\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_losses.append(loss.detach().cpu().numpy())\n",
    "    logging.info(f\"[step {pbar.n + 1}] loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "    pbar.set_description(f\"[step {pbar.n + 1}] loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "    if (pbar.n + 1) % SAVE_STEPS == 0:\n",
    "        \n",
    "        os.makedirs(f\"output/{MODEL_NAME}/epoch{1}-step{pbar.n}\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'losses': [train_losses],\n",
    "            }, f\"output/{MODEL_NAME}/epoch{1}-step{pbar.n+1}/model.pt\")\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"EVAL\")\n",
    "model.eval()\n",
    "eval_losses = []\n",
    "\n",
    "pbar = tqdm(eval_loader)\n",
    "for batch in pbar:\n",
    "    image_files, captions = batch\n",
    "    \n",
    "    loss = forward(model, mask_generator, tokenizer, cfg, param, image_files, captions)\n",
    "    eval_losses.append(loss.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model dict and loss\n",
    "\n",
    "os.makedirs(f\"output/{MODEL_NAME}/epoch{1}\", exist_ok=True)\n",
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'losses': [train_losses],\n",
    "            }, f\"output/{MODEL_NAME}/epoch{1}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev-sd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
